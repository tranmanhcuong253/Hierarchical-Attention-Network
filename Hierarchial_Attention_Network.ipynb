{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIN3gioqkQ1vCJyMDwaRV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tranmanhcuong253/Hierarchical-Attention-Network/blob/main/Hierarchial_Attention_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare Data"
      ],
      "metadata": {
        "id": "WEkWdoWBtYMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting GloVe for word embedding**"
      ],
      "metadata": {
        "id": "vjKDthUaStk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0bENWP0wUr_",
        "outputId": "7dff8221-82e6-48b4-bde6-1c926535dc2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-14 11:26:54--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-04-14 11:26:54--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-04-14 11:26:55--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.11MB/s    in 2m 39s  \n",
            "\n",
            "2024-04-14 11:29:34 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFOdiwStxHqs",
        "outputId": "3294932c-4c97-4a51-be46-edc032ba423f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWmWarsXxWyW",
        "outputId": "949fb178-c91a-40ad-c377-ca785533c61f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MsjdfUaxZI8A",
        "outputId": "ee04b32d-addb-41bc-bc74-910a92d6f4ad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TswvmowuDrj",
        "outputId": "5241b05a-dffd-4422-97b5-e9ae3e1ef095"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from sklearn.datasets  import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "AQULYTD-tqBH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create dataset from sklearn.fetch_20newsgroups dataset**\n",
        "\n",
        "Get the vocabulary set from the whole data. Using `transform()` method to encode each word to their relate index in GloVe list as numerical form"
      ],
      "metadata": {
        "id": "q4JWlj0GS8da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class News20Dataset(Dataset):\n",
        "  '''\n",
        "  A dataset class to be used in torch Dataloader to create batches\n",
        "  '''\n",
        "  def __init__(self,word_map_path,max_sent_length=150,max_doc_length=40,is_train=True):\n",
        "    \"\"\"\n",
        "    :param cache_data_path: folder where data files are stored\n",
        "    :param word_map_path: path for vocab dict, used for embedding\n",
        "    :param max_sent_length: maximum number of words in a sentence\n",
        "    :param max_doc_length: maximum number of sentences in a document\n",
        "    :param is_train: true if TRAIN mode, false if TEST mode\n",
        "    \"\"\"\n",
        "    self.max_sent_length = max_sent_length\n",
        "    self.max_doc_length = max_doc_length\n",
        "    self.split = 'train' if is_train else 'test'\n",
        "    #Get Dataset\n",
        "    self.data = fetch_20newsgroups(\n",
        "        subset=self.split,\n",
        "        categories=['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space'],\n",
        "        shuffle=False,\n",
        "        remove=('headers','footers','quotes')\n",
        "    )\n",
        "    #Get Vocabulary set\n",
        "    self.vocab = pd.read_csv(\n",
        "        filepath_or_buffer = word_map_path,\n",
        "        header=None,\n",
        "        sep=' ',\n",
        "        quoting=csv.QUOTE_NONE,\n",
        "        usecols=[0]).values[:50000]\n",
        "    self.vocab=['<pad>','<unk>'] + [word[0] for word in self.vocab]\n",
        "\n",
        "    #REFACTOR\n",
        "  def transform(self,text):\n",
        "    # encode document\n",
        "    doc = [\n",
        "        [self.vocab.index(word) if word in self.vocab else 1 for word in word_tokenize(text=sent)]\n",
        "        for sent in sent_tokenize(text=text)]  # if len(sent) > 0\n",
        "    doc = [sent[:self.max_sent_length] for sent in doc][:self.max_doc_length]\n",
        "    num_sents = min(len(doc), self.max_doc_length)\n",
        "\n",
        "    # skip erroneous ones\n",
        "    if num_sents == 0:\n",
        "        return None, -1, None\n",
        "\n",
        "    num_words = [min(len(sent), self.max_sent_length) for sent in doc][:self.max_doc_length]\n",
        "\n",
        "    return doc, num_sents, num_words\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    label = self.data['target'][i]\n",
        "    text = self.data['data'][i]\n",
        "\n",
        "    doc,num_sents,num_words=self.transform(text)\n",
        "\n",
        "    if num_sents==-1:\n",
        "      return None\n",
        "\n",
        "    return doc,label,num_sents,num_words\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.data['data'])\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self):\n",
        "      return len(self.vocab)\n",
        "\n",
        "  @property\n",
        "  def num_classes(self):\n",
        "      return 4\n",
        "      # return len(list(self.data.target_names))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QnDBbx9UuXec"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepares the batched data for input to a neural network model, ensuring that documents and sentences are properly represented as tensors and have the same size and shapes\n",
        "\n",
        "Create this function to use in `collate_fn` in DataLoader: a function that specifies how individual data samples should be batched together. When you have a dataset where each sample might have different shapes or sizes, you need to use `collate_fn` to customize the batching process."
      ],
      "metadata": {
        "id": "aV03QFNDunvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    batch = filter(lambda x: x is not None, batch)\n",
        "    docs, labels, doc_lengths, sent_lengths = list(zip(*batch))\n",
        "\n",
        "    bsz = len(labels)\n",
        "    batch_max_doc_length = max(doc_lengths)\n",
        "    batch_max_sent_length = max([max(sl) if sl else 0 for sl in sent_lengths])\n",
        "\n",
        "    docs_tensor = torch.zeros((bsz, batch_max_doc_length, batch_max_sent_length)).long()\n",
        "    sent_lengths_tensor = torch.zeros((bsz, batch_max_doc_length)).long()\n",
        "\n",
        "    for doc_idx, doc in enumerate(docs):\n",
        "        doc_length = doc_lengths[doc_idx]\n",
        "        sent_lengths_tensor[doc_idx, :doc_length] = torch.LongTensor(sent_lengths[doc_idx])\n",
        "        for sent_idx, sent in enumerate(doc):\n",
        "            sent_length = sent_lengths[doc_idx][sent_idx]\n",
        "            docs_tensor[doc_idx, sent_idx, :sent_length] = torch.LongTensor(sent)\n",
        "    return docs_tensor, torch.LongTensor(labels), torch.LongTensor(doc_lengths), sent_lengths_tensor\n"
      ],
      "metadata": {
        "id": "yYdV80Ma445c"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create our own DataLoader"
      ],
      "metadata": {
        "id": "3hSuJ7rM47FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom DataLoader to fit our own Dataset"
      ],
      "metadata": {
        "id": "08RJNjsLUA1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler"
      ],
      "metadata": {
        "id": "3cMo6e6O6m5M"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataLoader(DataLoader):\n",
        "  def __init__(self,dataset,batch_size):\n",
        "    self.n_samples = len(dataset)\n",
        "    self.sampler = RandomSampler(dataset)\n",
        "\n",
        "    self.init_kwargs = {\n",
        "        'dataset': dataset,\n",
        "        'batch_size': batch_size,\n",
        "        'pin_memory':True,\n",
        "        'collate_fn':collate_fn,\n",
        "        'shuffle':False # must be false to use sampler\n",
        "    }\n",
        "    super().__init__(sampler=self.sampler,**self.init_kwargs)"
      ],
      "metadata": {
        "id": "xti71wwr6yVC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = News20Dataset('glove.6B.100d.txt', is_train=True)\n",
        "data_loader=MyDataLoader(dataset,64)"
      ],
      "metadata": {
        "id": "RETv6RduF6Mz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (docs_tensor, labels, doc_lengths, sent_lengths_tensor) in enumerate(data_loader):\n",
        "    print(f\"Batch {batch_idx}:\")\n",
        "    print(f\"Batch size: {labels.size(0)}\")\n",
        "    print(f\"Docs tensor shape: {docs_tensor.shape}\")\n",
        "    print(f\"Docs tensor: {docs_tensor}\")\n",
        "    print(f\"Labels tensor shape: {labels.shape}\")\n",
        "    print(f\"Document lengths: {doc_lengths}\")\n",
        "    print(f\"Sentence lengths tensor shape: {sent_lengths_tensor.shape}\")\n",
        "    print(f\"Sentence lengths tensor: {sent_lengths_tensor}\")\n",
        "    if batch_idx%10==1:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3qZB4R6Gk9L",
        "outputId": "5ad57adf-bbbe-4d4f-b72b-827a56aad6c3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0:\n",
            "Batch size: 61\n",
            "Docs tensor shape: torch.Size([61, 40, 76])\n",
            "Docs tensor: tensor([[[    1,    56,    38,  ...,     0,     0,     0],\n",
            "         [    1,    56,    32,  ...,     0,     0,     0],\n",
            "         [    1, 13322,     1,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1, 41919,     7,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,    35,     2,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    1,     3,    63,  ...,     0,     0,     0],\n",
            "         [    1,   771,   410,  ...,     0,     0,     0],\n",
            "         [    1,   774,   191,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[   47,     1,   193,  ...,     0,     0,     0],\n",
            "         [    1,    22,   262,  ...,     0,     0,     0],\n",
            "         [    1,    22,  1448,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,    16,     9,  ...,     0,     0,     0],\n",
            "         [    1,    71,  3209,  ...,     0,     0,     0],\n",
            "         [    1,   474,   502,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]]])\n",
            "Labels tensor shape: torch.Size([61])\n",
            "Document lengths: tensor([ 4,  1,  1,  8,  5,  1,  8, 25,  3,  1,  9, 10,  2,  6,  3, 21,  2,  2,\n",
            "        32, 13,  1, 14,  5,  4,  5,  9, 12,  7,  1,  2, 10, 11, 28,  1,  2,  3,\n",
            "         6,  1,  6,  5, 11, 11,  6,  2,  5,  1,  2,  4,  5, 18, 40,  8, 29,  5,\n",
            "        13,  2,  6,  8,  8,  7, 15])\n",
            "Sentence lengths tensor shape: torch.Size([61, 40])\n",
            "Sentence lengths tensor: tensor([[18, 14,  4,  ...,  0,  0,  0],\n",
            "        [10,  0,  0,  ...,  0,  0,  0],\n",
            "        [23,  0,  0,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [43, 32,  8,  ...,  0,  0,  0],\n",
            "        [33, 12,  4,  ...,  0,  0,  0],\n",
            "        [12, 21, 17,  ...,  0,  0,  0]])\n",
            "Batch 1:\n",
            "Batch size: 60\n",
            "Docs tensor shape: torch.Size([60, 34, 150])\n",
            "Docs tensor: tensor([[[   47, 12259,     1,  ...,     0,     0,     0],\n",
            "         [   47, 12259,     1,  ...,     0,     0,     0],\n",
            "         [    1,  1546,    32,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,    47,     1,  ...,     0,     0,     0],\n",
            "         [    1,  3721,     7,  ...,     0,     0,     0],\n",
            "         [    1,   593,   132,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,    35,     9,  ...,     0,     0,     0],\n",
            "         [    1,   122,  1805,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[    1,    97,   826,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,  4031,     5,  ...,     0,     0,     0],\n",
            "         [    1,    34, 13140,  ...,     0,     0,     0],\n",
            "         [    1,  1546,    35,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
            "\n",
            "        [[    1,   410,     6,  ...,     0,     0,     0],\n",
            "         [    1,     3,     1,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         ...,\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0],\n",
            "         [    0,     0,     0,  ...,     0,     0,     0]]])\n",
            "Labels tensor shape: torch.Size([60])\n",
            "Document lengths: tensor([ 5, 11,  2,  3,  2, 11, 13,  8, 10,  4,  7, 12,  9,  2,  6,  4,  9, 16,\n",
            "         3,  1, 15,  7,  8,  3, 15,  1,  6, 15,  4, 28,  4,  8, 13, 34,  2,  4,\n",
            "         2,  4,  5,  6, 13, 12,  1,  3,  9, 13, 13,  3,  4,  1,  2,  4,  5,  2,\n",
            "         1, 16,  3,  1,  4,  2])\n",
            "Sentence lengths tensor shape: torch.Size([60, 34])\n",
            "Sentence lengths tensor: tensor([[15,  7,  8,  ...,  0,  0,  0],\n",
            "        [16, 21, 14,  ...,  0,  0,  0],\n",
            "        [ 5, 20,  0,  ...,  0,  0,  0],\n",
            "        ...,\n",
            "        [30,  0,  0,  ...,  0,  0,  0],\n",
            "        [18, 15,  6,  ...,  0,  0,  0],\n",
            "        [26, 20,  0,  ...,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Model\n",
        "https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf"
      ],
      "metadata": {
        "id": "HyxAUBwX7arF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence,PackedSequence"
      ],
      "metadata": {
        "id": "ZRz80KBs785M"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordAttention(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim,gru_hidden_dim,gru_num_layers, att_dim, use_layer_norm, dropout):\n",
        "    super(WordAttention,self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(vocab_size,embed_dim)\n",
        "\n",
        "    #output(batch,hidden_size)\n",
        "    self.gru = nn.GRU(embed_dim,gru_hidden_dim,num_layers=gru_num_layers,batch_first=True,bidirectional=True,\n",
        "                      dropout=dropout)\n",
        "    #Features\n",
        "    self.use_layer_norm = use_layer_norm\n",
        "    if use_layer_norm:\n",
        "      self.layer_norm = nn.LayerNorm(2 * gru_hidden_dim,elementwise_affine=True)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "     # Maps gru output to `att_dim` sized tensor\n",
        "    self.attention = nn.Linear(2 * gru_hidden_dim, att_dim)\n",
        "\n",
        "    # Word context vector (u_w) to take dot-product with\n",
        "    self.context_vector = nn.Linear(att_dim, 1, bias=False)\n",
        "\n",
        "  def init_embeddings(self, embeddings):\n",
        "    \"\"\"\n",
        "    Initialized embedding layer with pretrained embeddings.\n",
        "    :param embeddings: embeddings to init with\n",
        "    \"\"\"\n",
        "    # NOTE MODIFICATION (EMBEDDING)\n",
        "    self.embeddings.weight = nn.Parameter(embeddings)\n",
        "\n",
        "  def freeze_embeddings(self, freeze=False):\n",
        "    \"\"\"\n",
        "    Set whether to freeze pretrained embeddings.\n",
        "    :param freeze: True to freeze weights\n",
        "    \"\"\"\n",
        "    # NOTE MODIFICATION (EMBEDDING)\n",
        "    self.embeddings.weight.requires_grad = not freeze\n",
        "\n",
        "  def forward(self, sents, sent_lengths):\n",
        "    \"\"\"\n",
        "    :param sents: encoded sentence-level data; LongTensor (num_sents, pad_len, embed_dim)\n",
        "    :param sent_lengths: sentence lengths; LongTensor (num_sents)\n",
        "    :return: sentence embeddings, attention weights of words\n",
        "    \"\"\"\n",
        "    # Sort sents by decreasing order in sentence lengths\n",
        "    sent_lengths, sent_perm_idx = sent_lengths.sort(dim=0, descending=True)\n",
        "    sents = sents[sent_perm_idx]\n",
        "\n",
        "    sents = self.embeddings(sents)\n",
        "    sents = self.dropout(sents)\n",
        "\n",
        "    packed_words = pack_padded_sequence(sents, lengths=sent_lengths.tolist(), batch_first=True)\n",
        "\n",
        "    # effective batch size at each timestep\n",
        "    valid_bsz = packed_words.batch_sizes\n",
        "\n",
        "    # Apply word-level GRU over word embeddings\n",
        "    packed_words, _ = self.gru(packed_words)\n",
        "\n",
        "    # NOTE MODIFICATION (FEATURES)\n",
        "    if self.use_layer_norm:\n",
        "        normed_words = self.layer_norm(packed_words.data)\n",
        "    else:\n",
        "        normed_words = packed_words\n",
        "\n",
        "    # Word Attenton\n",
        "    att = torch.tanh(self.attention(normed_words.data))\n",
        "    att = self.context_vector(att).squeeze(1)\n",
        "\n",
        "    val = att.max()\n",
        "    att = torch.exp(att - val) # att.size: (n_words)\n",
        "    #subtracts the maximum value val from each element of att and then applies the exponential function element-wise to the result.\n",
        "    #This operation is often used for numerical stability when dealing with probabilities or softmax operations.\n",
        "    #It's a common technique to prevent overflow issues that can occur when exponentiating large numbers.\n",
        "\n",
        "    # Restore as sentences by repadding\n",
        "    att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
        "\n",
        "    # NOTE MODIFICATION (BUG) : attention score sum should be in dimension\n",
        "    att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
        "\n",
        "    # Restore as sentences by repadding\n",
        "    sents, _ = pad_packed_sequence(packed_words, batch_first=True)\n",
        "\n",
        "    # Compute sentence vectors\n",
        "    sents = sents * att_weights.unsqueeze(2)\n",
        "    sents = sents.sum(dim=1)\n",
        "\n",
        "    # Restore the original order of sentences (undo the first sorting)\n",
        "    _, sent_unperm_idx = sent_perm_idx.sort(dim=0, descending=False)\n",
        "    sents = sents[sent_unperm_idx]\n",
        "\n",
        "    # NOTE MODIFICATION BUG\n",
        "    att_weights = att_weights[sent_unperm_idx]\n",
        "\n",
        "    return sents, att_weights"
      ],
      "metadata": {
        "id": "uJfVRkom02xW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Sentence-level attention module. Contains a word-level attention module.\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
        "              word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout):\n",
        "      super(SentenceAttention, self).__init__()\n",
        "\n",
        "      # Word-level attention module\n",
        "      self.word_attention = WordAttention(vocab_size, embed_dim, word_gru_hidden_dim, word_gru_num_layers,\n",
        "                                          word_att_dim, use_layer_norm, dropout)\n",
        "\n",
        "      # Bidirectional sentence-level GRU\n",
        "      self.gru = nn.GRU(2 * word_gru_hidden_dim, sent_gru_hidden_dim, num_layers=sent_gru_num_layers,\n",
        "                        batch_first=True, bidirectional=True, dropout=dropout)\n",
        "\n",
        "      # NOTE MODIFICATION (FEATURES)\n",
        "      self.use_layer_norm = use_layer_norm\n",
        "      if use_layer_norm:\n",
        "          self.layer_norm = nn.LayerNorm(2 * sent_gru_hidden_dim, elementwise_affine=True)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      # Sentence-level attention\n",
        "      self.sent_attention = nn.Linear(2 * sent_gru_hidden_dim, sent_att_dim)\n",
        "\n",
        "      # Sentence context vector u_s to take dot product with\n",
        "      # This is equivalent to taking that dot product (Eq.10 in the paper),\n",
        "      # as u_s is the linear layer's 1D parameter vector here\n",
        "      self.sentence_context_vector = nn.Linear(sent_att_dim, 1, bias=False)\n",
        "\n",
        "  def forward(self, docs, doc_lengths, sent_lengths):\n",
        "      \"\"\"\n",
        "      :param docs: encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "      :param doc_lengths: unpadded document lengths; LongTensor (num_docs)\n",
        "      :param sent_lengths: unpadded sentence lengths; LongTensor (num_docs, padded_doc_length)\n",
        "      :return: document embeddings, attention weights of words, attention weights of sentences\n",
        "      \"\"\"\n",
        "      # Sort documents by decreasing order in length\n",
        "      doc_lengths, doc_perm_idx = doc_lengths.sort(dim=0, descending=True)\n",
        "      docs = docs[doc_perm_idx]\n",
        "      sent_lengths = sent_lengths[doc_perm_idx]\n",
        "\n",
        "      # Make a long batch of sentences by removing pad-sentences\n",
        "      # i.e. `docs` was of size (num_docs, padded_doc_length, padded_sent_length)\n",
        "      # -> `packed_sents.data` is now of size (num_sents, padded_sent_length)\n",
        "      packed_sents = pack_padded_sequence(docs, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "\n",
        "      # effective batch size at each timestep\n",
        "      valid_bsz = packed_sents.batch_sizes\n",
        "\n",
        "      # Make a long batch of sentence lengths by removing pad-sentences\n",
        "      # i.e. `sent_lengths` was of size (num_docs, padded_doc_length)\n",
        "      # -> `packed_sent_lengths.data` is now of size (num_sents)\n",
        "      packed_sent_lengths = pack_padded_sequence(sent_lengths, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "\n",
        "      # Word attention module\n",
        "      sents, word_att_weights = self.word_attention(packed_sents.data, packed_sent_lengths.data)\n",
        "\n",
        "      # NOTE MODIFICATION (FEATURES)\n",
        "      sents = self.dropout(sents)\n",
        "\n",
        "      # Sentence-level GRU over sentence embeddings\n",
        "      packed_sents, _ = self.gru(PackedSequence(sents, valid_bsz))\n",
        "\n",
        "      # NOTE MODIFICATION (FEATURES)\n",
        "      if self.use_layer_norm:\n",
        "          normed_sents = self.layer_norm(packed_sents.data)\n",
        "      else:\n",
        "          normed_sents = packed_sents\n",
        "\n",
        "      # Sentence attention\n",
        "      att = torch.tanh(self.sent_attention(normed_sents))\n",
        "      att = self.sentence_context_vector(att).squeeze(1)\n",
        "\n",
        "      # NOTE MODIFICATION (BUG)\n",
        "      val = att.max()\n",
        "      att = torch.exp(att - val)\n",
        "\n",
        "      # Restore as documents by repadding\n",
        "      att, _ = pad_packed_sequence(PackedSequence(att, valid_bsz), batch_first=True)\n",
        "\n",
        "      # Note MODIFICATION (BUG)\n",
        "      sent_att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
        "\n",
        "      # Restore as documents by repadding\n",
        "      docs, _ = pad_packed_sequence(packed_sents, batch_first=True)\n",
        "\n",
        "      # Compute document vectors\n",
        "      docs = docs * sent_att_weights.unsqueeze(2)\n",
        "      docs = docs.sum(dim=1)\n",
        "\n",
        "      # Restore as documents by repadding\n",
        "      word_att_weights, _ = pad_packed_sequence(PackedSequence(word_att_weights, valid_bsz), batch_first=True)\n",
        "\n",
        "      # Restore the original order of documents (undo the first sorting)\n",
        "      _, doc_unperm_idx = doc_perm_idx.sort(dim=0, descending=False)\n",
        "      docs = docs[doc_unperm_idx]\n",
        "\n",
        "      # NOTE MODIFICATION (BUG)\n",
        "      word_att_weights = word_att_weights[doc_unperm_idx]\n",
        "      sent_att_weights = sent_att_weights[doc_unperm_idx]\n",
        "\n",
        "      return docs, word_att_weights, sent_att_weights"
      ],
      "metadata": {
        "id": "Qc-NnIZ_DhVl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalAttentionNetwork(nn.Module):\n",
        "    def __init__(self, num_classes, vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
        "                word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout):\n",
        "        \"\"\"\n",
        "        :param num_classes: number of classes\n",
        "        :param vocab_size: number of words in the vocabulary of the model\n",
        "        :param embed_dim: dimension of word embeddings\n",
        "        :param word_gru_hidden_dim: dimension of word-level GRU; biGRU output is double this size\n",
        "        :param sent_gru_hidden_dim: dimension of sentence-level GRU; biGRU output is double this size\n",
        "        :param word_gru_num_layers: number of layers in word-level GRU\n",
        "        :param sent_gru_num_layers: number of layers in sentence-level GRU\n",
        "        :param word_att_dim: dimension of word-level attention layer\n",
        "        :param sent_att_dim: dimension of sentence-level attention layer\n",
        "        :param use_layer_norm: whether to use layer normalization\n",
        "        :param dropout: dropout rate; 0 to not use dropout\n",
        "        \"\"\"\n",
        "        super(HierarchicalAttentionNetwork, self).__init__()\n",
        "\n",
        "        self.sent_attention = SentenceAttention(\n",
        "            vocab_size, embed_dim, word_gru_hidden_dim, sent_gru_hidden_dim,\n",
        "            word_gru_num_layers, sent_gru_num_layers, word_att_dim, sent_att_dim, use_layer_norm, dropout)\n",
        "\n",
        "        # classifier\n",
        "        self.fc = nn.Linear(2 * sent_gru_hidden_dim, num_classes)\n",
        "\n",
        "        # NOTE MODIFICATION (FEATURES)\n",
        "        self.use_layer_nome = use_layer_norm\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, docs, doc_lengths, sent_lengths):\n",
        "        \"\"\"\n",
        "        :param docs: encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :param doc_lengths: unpadded document lengths; LongTensor (num_docs)\n",
        "        :param sent_lengths: unpadded sentence lengths; LongTensor (num_docs, max_sent_len)\n",
        "        :return: class scores, attention weights of words, attention weights of sentences\n",
        "        \"\"\"\n",
        "        doc_embeds, word_att_weights, sent_att_weights = self.sent_attention(docs, doc_lengths, sent_lengths)\n",
        "\n",
        "        scores = self.fc(doc_embeds)\n",
        "\n",
        "        return scores, word_att_weights, sent_att_weights"
      ],
      "metadata": {
        "id": "PHUN2GPAE8ee"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utility Function"
      ],
      "metadata": {
        "id": "9ppdrrWWF_c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pylab import *\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "-sWwB80TJvhI"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8OJPmw6YJ1MO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricTracker(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, summed_val, n=1):\n",
        "        self.val = summed_val / n\n",
        "        self.sum += summed_val\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "Sj68Wl2pJ5lv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the pretrained weights of GloVe, the vector embedding form of every word in data**"
      ],
      "metadata": {
        "id": "fYhyauo-U1vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pretrained_weights( corpus_vocab, embed_dim, device):\n",
        "    \"\"\"\n",
        "    Returns 50002 words' pretrained weights in tensor\n",
        "    :param corpus_vocab: vocabulary from dataset\n",
        "    :return: tensor (len(vocab), embed_dim)\n",
        "    \"\"\"\n",
        "    save_dir = os.path.join( 'glove_pretrained.pt')\n",
        "    if os.path.exists(save_dir):\n",
        "        return torch.load(save_dir, map_location=device)\n",
        "\n",
        "    corpus_set = set(corpus_vocab)\n",
        "    pretrained_vocab = set()\n",
        "    glove_pretrained = torch.zeros(len(corpus_vocab), embed_dim)\n",
        "    with open(os.path.join( 'glove.6B.100d.txt'), 'rb') as f:\n",
        "        for l in tqdm(f):\n",
        "            line = l.decode().split()\n",
        "            if line[0] in corpus_set:\n",
        "                pretrained_vocab.add(line[0])\n",
        "                glove_pretrained[corpus_vocab.index(line[0])] = torch.from_numpy(np.array(line[1:]).astype(float))\n",
        "\n",
        "        # handling 'out of vocabulary' words\n",
        "        var = float(torch.var(glove_pretrained))\n",
        "        for oov in corpus_set.difference(pretrained_vocab):\n",
        "            glove_pretrained[corpus_vocab.index(oov)] = torch.empty(100).float().uniform_(-var, var)\n",
        "        print(\"weight size:\", glove_pretrained.size())\n",
        "        torch.save(glove_pretrained, save_dir)\n",
        "    return glove_pretrained\n"
      ],
      "metadata": {
        "id": "jGRwXi1QKxv3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions for visuallization of the prediction**"
      ],
      "metadata": {
        "id": "nVRwPxtmVIGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sentence_to_color(words, scores, sent_score):\n",
        "    \"\"\"\n",
        "    :param words: array of words\n",
        "    :param scores: array of attention scores each corresponding to a word\n",
        "    :param sent_score: sentence attention score\n",
        "    :return: html formatted string\n",
        "    \"\"\"\n",
        "\n",
        "    sentencemap = matplotlib.cm.get_cmap('binary')\n",
        "    wordmap = matplotlib.cm.get_cmap('OrRd')\n",
        "    result = '<p><span style=\"margin:5px; padding:5px; background-color: {}\">'\\\n",
        "        .format(matplotlib.colors.rgb2hex(sentencemap(sent_score)[:3]))\n",
        "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
        "    for word, score in zip(words, scores):\n",
        "        color = matplotlib.colors.rgb2hex(wordmap(score)[:3])\n",
        "        result += template.format(color, '&nbsp' + word + '&nbsp')\n",
        "    result += '</span><p>'\n",
        "    return result"
      ],
      "metadata": {
        "id": "XnnDs8oPLb0P"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bar_chart(categories, scores, graph_title='Prediction', output_name='prediction_bar_chart.png'):\n",
        "    y_pos = arange(len(categories))\n",
        "\n",
        "    plt.bar(y_pos, scores, align='center', alpha=0.5)\n",
        "    plt.xticks(y_pos, categories)\n",
        "    plt.ylabel('Attention Score')\n",
        "    plt.title(graph_title)\n",
        "\n",
        "    plt.gca().spines['top'].set_visible(False)\n",
        "    plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "    plt.savefig(output_name)"
      ],
      "metadata": {
        "id": "tDaqFVMoLezO"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(model, dataset, doc):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \"\"\"\n",
        "    # Predicts, and visualizes one document with html file\n",
        "    :param model: pretrained model\n",
        "    :param dataset: news20 dataset\n",
        "    :param doc: document to feed in\n",
        "    :return: html formatted string for whole document\n",
        "    \"\"\"\n",
        "\n",
        "    orig_doc = [word_tokenize(sent) for sent in sent_tokenize(doc)]\n",
        "    doc, num_sents, num_words = dataset.transform(doc)\n",
        "    label = 0  # dummy label for transformation\n",
        "\n",
        "    doc, label, doc_length, sent_length = collate_fn([(doc, label, num_sents, num_words)])\n",
        "    score, word_att_weight, sentence_att_weight \\\n",
        "        = model(doc.to(device), doc_length.to(device), sent_length.to(device))\n",
        "\n",
        "    # predicted = int(torch.max(score, dim=1)[1])\n",
        "    classes = ['Cryptography', 'Electronics', 'Medical', 'Space']\n",
        "    result = \"<h2>Attention Visualization</h2>\"\n",
        "\n",
        "    bar_chart(classes, torch.softmax(score.detach(), dim=1).flatten().cpu(), 'Prediction')\n",
        "    result += '<br><img src=\"prediction_bar_chart.png\"><br>'\n",
        "    for orig_sent, att_weight, sent_weight in zip(orig_doc, word_att_weight[0].tolist(), sentence_att_weight[0].tolist()):\n",
        "        result += map_sentence_to_color(orig_sent, att_weight, sent_weight)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "9d-N9pcwLg7b"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "p4Sw9N-4LjL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class `Evaluation` using for the validation of the model after the training in each epoch. If the epoch's model when using for the validation has the best accuracy, it will be selected as the model to do the prediction in test section"
      ],
      "metadata": {
        "id": "PKVw9CKYY85E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluation:\n",
        "    def __init__(self, config, model):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.device = next(self.model.parameters()).device\n",
        "\n",
        "        self.dataset = News20Dataset(config['vocab_path'], is_train=False)\n",
        "        self.dataloader = torch.utils.data.DataLoader(self.dataset, batch_size=config['batch_size'], shuffle=False,\n",
        "                                                      collate_fn=collate_fn)\n",
        "\n",
        "        self.accs = MetricTracker()\n",
        "        self.best_acc = 0\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            self.accs.reset()\n",
        "\n",
        "            for (docs, labels, doc_lengths, sent_lengths) in self.dataloader:\n",
        "                batch_size = labels.size(0)\n",
        "\n",
        "                docs = docs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                doc_lengths = doc_lengths.to(self.device)\n",
        "                sent_lengths = sent_lengths.to(self.device)\n",
        "\n",
        "                scores, word_att_weights, sentence_att_weights = self.model(docs, doc_lengths, sent_lengths)\n",
        "\n",
        "                predictions = scores.max(dim=1)[1]\n",
        "                #returns a tuple containing two tensors: the maximum values and their corresponding indices along the specified dimension.\n",
        "                #[1]: This index [1] selects the second element of the tuple returned by max(dim=1), which contains the indices of the maximum values.\n",
        "                correct_predictions = torch.eq(predictions, labels).sum().item()\n",
        "                acc = correct_predictions\n",
        "\n",
        "                self.accs.update(acc, batch_size)\n",
        "            self.best_acc = max(self.best_acc, self.accs.avg)\n",
        "\n",
        "            print('Test Average Accuracy: {acc.avg:.4f}'.format(acc=self.accs))\n"
      ],
      "metadata": {
        "id": "QeSzVjOhLtJv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trainer Class describes what do we need to do during training session**"
      ],
      "metadata": {
        "id": "RjagO4NOZxW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, config, model, optimizer, criterion, dataloader):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.dataloader = dataloader\n",
        "        self.device = next(self.model.parameters()).device\n",
        "\n",
        "        self.losses = MetricTracker()\n",
        "        self.accs = MetricTracker()\n",
        "\n",
        "        self.tester = Evaluation(self.config, self.model)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.config['num_epochs']):\n",
        "            result = self._train_epoch(epoch)\n",
        "            print('Epoch: [{0}]\\t Avg Loss {loss:.4f}\\t Avg Accuracy {acc:.3f}'.format(epoch, loss=result['loss'], acc=result['acc']))\n",
        "            # NOTE MODIFICATION (TEST)\n",
        "            self.tester.eval()\n",
        "\n",
        "            if self.tester.best_acc == self.tester.accs.avg:\n",
        "                print('Saving Model...')\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model': self.model,\n",
        "                    'optimizer': self.optimizer,\n",
        "                }, 'best_model/model.pth.tar')\n",
        "\n",
        "    def _train_epoch(self, epoch_idx):\n",
        "        self.model.train()\n",
        "\n",
        "        self.losses.reset()\n",
        "        self.accs.reset()\n",
        "\n",
        "        for batch_idx, (docs, labels, doc_lengths, sent_lengths) in enumerate(self.dataloader):\n",
        "            batch_size = labels.size(0)\n",
        "\n",
        "            docs = docs.to(self.device)  # (batch_size, padded_doc_length, padded_sent_length)\n",
        "            labels = labels.to(self.device)  # (batch_size)\n",
        "            sent_lengths = sent_lengths.to(self.device)  # (batch_size, padded_doc_length)\n",
        "            doc_lengths = doc_lengths.to(self.device)  # (batch_size)\n",
        "\n",
        "            # (n_docs, n_classes), (n_docs, max_doc_len_in_batch, max_sent_len_in_batch), (n_docs, max_doc_len_in_batch)\n",
        "            scores, word_att_weights, sentence_att_weights = self.model(docs, doc_lengths, sent_lengths)\n",
        "\n",
        "            # NOTE MODIFICATION (BUG)\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            loss = self.criterion(scores, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            if self.config['max_grad_norm'] is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])\n",
        "\n",
        "            # NOTE MODIFICATION (BUG): clip grad norm should come before optimizer.step()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Compute accuracy\n",
        "            predictions = scores.max(dim=1)[1]\n",
        "            correct_predictions = torch.eq(predictions, labels).sum().item()\n",
        "            acc = correct_predictions\n",
        "\n",
        "            self.losses.update(loss.item(), batch_size)\n",
        "            self.accs.update(acc, batch_size)\n",
        "\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t Loss {loss.val:.4f}(avg: {loss.avg:.4f})\\t Acc {acc.val:.3f} (avg: {acc.avg:.3f})'.format(\n",
        "                    epoch_idx, batch_idx, len(self.dataloader), loss=self.losses, acc=self.accs))\n",
        "\n",
        "        log = {'loss': self.losses.avg, 'acc': self.accs.avg}\n",
        "        return log"
      ],
      "metadata": {
        "id": "tI5lUXK7OjFs"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "UvnVOXylPB6S"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`train()` function put together everything we need, create dataset, dataloader and train the model through `Trainer` class"
      ],
      "metadata": {
        "id": "9dJwdzrLZ7ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config, device):\n",
        "    dataset = News20Dataset(config['vocab_path'], is_train=True)\n",
        "\n",
        "    dataloader = MyDataLoader(dataset, config['batch_size'])\n",
        "\n",
        "    model = HierarchicalAttentionNetwork(\n",
        "        num_classes=dataset.num_classes,\n",
        "        vocab_size=dataset.vocab_size,\n",
        "        embed_dim=config['embed_dim'],\n",
        "        word_gru_hidden_dim=config['word_gru_hidden_dim'],\n",
        "        sent_gru_hidden_dim=config['sent_gru_hidden_dim'],\n",
        "        word_gru_num_layers=config['word_gru_num_layers'],\n",
        "        sent_gru_num_layers=config['sent_gru_num_layers'],\n",
        "        word_att_dim=config['word_att_dim'],\n",
        "        sent_att_dim=config['sent_att_dim'],\n",
        "        use_layer_norm=config['use_layer_norm'],\n",
        "        dropout=config['dropout']).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=config['lr'])\n",
        "\n",
        "    # NOTE MODIFICATION (BUG)\n",
        "    # criterion = nn.NLLLoss(reduction='sum').to(device) # option 1\n",
        "    criterion = nn.CrossEntropyLoss(reduction='sum').to(device)  # option 2\n",
        "\n",
        "    # NOTE MODIFICATION (EMBEDDING)\n",
        "    if config['pretrain']:\n",
        "        weights = get_pretrained_weights(dataset.vocab, config['embed_dim'], device)\n",
        "        model.sent_attention.word_attention.init_embeddings(weights)\n",
        "    model.sent_attention.word_attention.freeze_embeddings(config['freeze'])\n",
        "\n",
        "    trainer = Trainer(config, model, optimizer, criterion, dataloader)\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "kdXYebmQPIl7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('best_model', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "M9M8UHTPWltr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "config = {\n",
        "    \"batch_size\": 64,\n",
        "    \"num_epochs\": 25,\n",
        "    \"lr\": 3e-3,\n",
        "    \"max_grad_norm\": 5,\n",
        "    \"embed_dim\": 100,\n",
        "    \"word_gru_hidden_dim\": 100,\n",
        "    \"sent_gru_hidden_dim\": 100,\n",
        "    \"word_gru_num_layers\": 1,\n",
        "    \"sent_gru_num_layers\": 1,\n",
        "    \"word_att_dim\": 200,\n",
        "    \"sent_att_dim\": 200,\n",
        "    \"vocab_path\": \"glove.6B.100d.txt\",\n",
        "    \"pretrain\": True,\n",
        "    \"freeze\": False,\n",
        "    \"use_layer_norm\": True,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "train(config, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9wDXpJRPMSq",
        "outputId": "8d2819ea-ad4a-40e8-cf63-9a35eeeb2a7c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/38]\t Loss 1.3729(avg: 1.3729)\t Acc 0.397 (avg: 0.397)\n",
            "Epoch: [0][1/38]\t Loss 1.5265(avg: 1.4497)\t Acc 0.238 (avg: 0.317)\n",
            "Epoch: [0][2/38]\t Loss 1.4789(avg: 1.4592)\t Acc 0.230 (avg: 0.289)\n",
            "Epoch: [0][3/38]\t Loss 1.4160(avg: 1.4485)\t Acc 0.290 (avg: 0.289)\n",
            "Epoch: [0][4/38]\t Loss 1.3681(avg: 1.4324)\t Acc 0.258 (avg: 0.283)\n",
            "Epoch: [0][5/38]\t Loss 1.3536(avg: 1.4190)\t Acc 0.250 (avg: 0.277)\n",
            "Epoch: [0][6/38]\t Loss 1.3212(avg: 1.4049)\t Acc 0.397 (avg: 0.295)\n",
            "Epoch: [0][7/38]\t Loss 1.3986(avg: 1.4041)\t Acc 0.323 (avg: 0.298)\n",
            "Epoch: [0][8/38]\t Loss 1.3129(avg: 1.3942)\t Acc 0.426 (avg: 0.312)\n",
            "Epoch: [0][9/38]\t Loss 1.2894(avg: 1.3836)\t Acc 0.381 (avg: 0.319)\n",
            "Epoch: [0][10/38]\t Loss 1.1960(avg: 1.3667)\t Acc 0.435 (avg: 0.329)\n",
            "Epoch: [0][11/38]\t Loss 1.1044(avg: 1.3449)\t Acc 0.548 (avg: 0.348)\n",
            "Epoch: [0][12/38]\t Loss 1.1947(avg: 1.3340)\t Acc 0.475 (avg: 0.357)\n",
            "Epoch: [0][13/38]\t Loss 1.0432(avg: 1.3129)\t Acc 0.619 (avg: 0.376)\n",
            "Epoch: [0][14/38]\t Loss 0.9440(avg: 1.2876)\t Acc 0.594 (avg: 0.391)\n",
            "Epoch: [0][15/38]\t Loss 1.0569(avg: 1.2737)\t Acc 0.633 (avg: 0.405)\n",
            "Epoch: [0][16/38]\t Loss 1.1064(avg: 1.2639)\t Acc 0.581 (avg: 0.416)\n",
            "Epoch: [0][17/38]\t Loss 1.0731(avg: 1.2531)\t Acc 0.540 (avg: 0.423)\n",
            "Epoch: [0][18/38]\t Loss 0.9102(avg: 1.2351)\t Acc 0.661 (avg: 0.435)\n",
            "Epoch: [0][19/38]\t Loss 0.8312(avg: 1.2144)\t Acc 0.562 (avg: 0.442)\n",
            "Epoch: [0][20/38]\t Loss 0.8877(avg: 1.1989)\t Acc 0.645 (avg: 0.451)\n",
            "Epoch: [0][21/38]\t Loss 0.7413(avg: 1.1778)\t Acc 0.683 (avg: 0.462)\n",
            "Epoch: [0][22/38]\t Loss 0.9235(avg: 1.1670)\t Acc 0.541 (avg: 0.465)\n",
            "Epoch: [0][23/38]\t Loss 0.8003(avg: 1.1520)\t Acc 0.639 (avg: 0.473)\n",
            "Epoch: [0][24/38]\t Loss 0.8424(avg: 1.1396)\t Acc 0.645 (avg: 0.479)\n",
            "Epoch: [0][25/38]\t Loss 0.8604(avg: 1.1289)\t Acc 0.710 (avg: 0.488)\n",
            "Epoch: [0][26/38]\t Loss 0.6728(avg: 1.1116)\t Acc 0.781 (avg: 0.499)\n",
            "Epoch: [0][27/38]\t Loss 0.8606(avg: 1.1023)\t Acc 0.656 (avg: 0.505)\n",
            "Epoch: [0][28/38]\t Loss 0.7610(avg: 1.0904)\t Acc 0.683 (avg: 0.511)\n",
            "Epoch: [0][29/38]\t Loss 0.6397(avg: 1.0750)\t Acc 0.734 (avg: 0.519)\n",
            "Epoch: [0][30/38]\t Loss 0.6959(avg: 1.0629)\t Acc 0.758 (avg: 0.527)\n",
            "Epoch: [0][31/38]\t Loss 0.7031(avg: 1.0517)\t Acc 0.694 (avg: 0.532)\n",
            "Epoch: [0][32/38]\t Loss 0.8714(avg: 1.0462)\t Acc 0.667 (avg: 0.536)\n",
            "Epoch: [0][33/38]\t Loss 0.5730(avg: 1.0325)\t Acc 0.803 (avg: 0.544)\n",
            "Epoch: [0][34/38]\t Loss 0.7496(avg: 1.0242)\t Acc 0.703 (avg: 0.548)\n",
            "Epoch: [0][35/38]\t Loss 0.8655(avg: 1.0199)\t Acc 0.607 (avg: 0.550)\n",
            "Epoch: [0][36/38]\t Loss 1.0344(avg: 1.0203)\t Acc 0.625 (avg: 0.552)\n",
            "Epoch: [0][37/38]\t Loss 0.6190(avg: 1.0196)\t Acc 0.500 (avg: 0.552)\n",
            "Epoch: [0]\t Avg Loss 1.0196\t Avg Accuracy 0.552\n",
            "Test Average Accuracy: 0.6238\n",
            "Saving Model...\n",
            "Epoch: [1][0/38]\t Loss 0.5829(avg: 0.5829)\t Acc 0.730 (avg: 0.730)\n",
            "Epoch: [1][1/38]\t Loss 0.9179(avg: 0.7504)\t Acc 0.635 (avg: 0.683)\n",
            "Epoch: [1][2/38]\t Loss 0.7738(avg: 0.7581)\t Acc 0.710 (avg: 0.691)\n",
            "Epoch: [1][3/38]\t Loss 0.7076(avg: 0.7454)\t Acc 0.714 (avg: 0.697)\n",
            "Epoch: [1][4/38]\t Loss 0.6367(avg: 0.7242)\t Acc 0.770 (avg: 0.712)\n",
            "Epoch: [1][5/38]\t Loss 0.7382(avg: 0.7265)\t Acc 0.689 (avg: 0.708)\n",
            "Epoch: [1][6/38]\t Loss 0.9179(avg: 0.7530)\t Acc 0.650 (avg: 0.700)\n",
            "Epoch: [1][7/38]\t Loss 0.7805(avg: 0.7563)\t Acc 0.717 (avg: 0.702)\n",
            "Epoch: [1][8/38]\t Loss 0.6533(avg: 0.7448)\t Acc 0.677 (avg: 0.699)\n",
            "Epoch: [1][9/38]\t Loss 0.7104(avg: 0.7413)\t Acc 0.688 (avg: 0.698)\n",
            "Epoch: [1][10/38]\t Loss 0.5809(avg: 0.7269)\t Acc 0.803 (avg: 0.707)\n",
            "Epoch: [1][11/38]\t Loss 0.7460(avg: 0.7285)\t Acc 0.661 (avg: 0.704)\n",
            "Epoch: [1][12/38]\t Loss 0.6250(avg: 0.7205)\t Acc 0.774 (avg: 0.709)\n",
            "Epoch: [1][13/38]\t Loss 0.5699(avg: 0.7096)\t Acc 0.794 (avg: 0.715)\n",
            "Epoch: [1][14/38]\t Loss 0.6605(avg: 0.7062)\t Acc 0.714 (avg: 0.715)\n",
            "Epoch: [1][15/38]\t Loss 0.6717(avg: 0.7040)\t Acc 0.762 (avg: 0.718)\n",
            "Epoch: [1][16/38]\t Loss 0.6102(avg: 0.6987)\t Acc 0.800 (avg: 0.723)\n",
            "Epoch: [1][17/38]\t Loss 0.5210(avg: 0.6885)\t Acc 0.781 (avg: 0.726)\n",
            "Epoch: [1][18/38]\t Loss 0.7641(avg: 0.6925)\t Acc 0.683 (avg: 0.724)\n",
            "Epoch: [1][19/38]\t Loss 0.7166(avg: 0.6938)\t Acc 0.703 (avg: 0.723)\n",
            "Epoch: [1][20/38]\t Loss 0.5522(avg: 0.6871)\t Acc 0.790 (avg: 0.726)\n",
            "Epoch: [1][21/38]\t Loss 0.5915(avg: 0.6829)\t Acc 0.783 (avg: 0.728)\n",
            "Epoch: [1][22/38]\t Loss 0.6557(avg: 0.6817)\t Acc 0.698 (avg: 0.727)\n",
            "Epoch: [1][23/38]\t Loss 0.9199(avg: 0.6916)\t Acc 0.613 (avg: 0.722)\n",
            "Epoch: [1][24/38]\t Loss 0.8742(avg: 0.6990)\t Acc 0.635 (avg: 0.719)\n",
            "Epoch: [1][25/38]\t Loss 0.7712(avg: 0.7018)\t Acc 0.672 (avg: 0.717)\n",
            "Epoch: [1][26/38]\t Loss 0.6462(avg: 0.6997)\t Acc 0.683 (avg: 0.716)\n",
            "Epoch: [1][27/38]\t Loss 0.5538(avg: 0.6946)\t Acc 0.774 (avg: 0.718)\n",
            "Epoch: [1][28/38]\t Loss 0.6186(avg: 0.6919)\t Acc 0.778 (avg: 0.720)\n",
            "Epoch: [1][29/38]\t Loss 0.4696(avg: 0.6844)\t Acc 0.825 (avg: 0.723)\n",
            "Epoch: [1][30/38]\t Loss 0.6696(avg: 0.6839)\t Acc 0.734 (avg: 0.724)\n",
            "Epoch: [1][31/38]\t Loss 0.5829(avg: 0.6808)\t Acc 0.754 (avg: 0.725)\n",
            "Epoch: [1][32/38]\t Loss 0.3713(avg: 0.6716)\t Acc 0.885 (avg: 0.729)\n",
            "Epoch: [1][33/38]\t Loss 0.5530(avg: 0.6681)\t Acc 0.778 (avg: 0.731)\n",
            "Epoch: [1][34/38]\t Loss 0.6422(avg: 0.6674)\t Acc 0.750 (avg: 0.731)\n",
            "Epoch: [1][35/38]\t Loss 0.5418(avg: 0.6638)\t Acc 0.794 (avg: 0.733)\n",
            "Epoch: [1][36/38]\t Loss 0.5535(avg: 0.6609)\t Acc 0.790 (avg: 0.735)\n",
            "Epoch: [1][37/38]\t Loss 0.6593(avg: 0.6609)\t Acc 0.600 (avg: 0.734)\n",
            "Epoch: [1]\t Avg Loss 0.6609\t Avg Accuracy 0.734\n",
            "Test Average Accuracy: 0.7177\n",
            "Saving Model...\n",
            "Epoch: [2][0/38]\t Loss 0.4304(avg: 0.4304)\t Acc 0.844 (avg: 0.844)\n",
            "Epoch: [2][1/38]\t Loss 0.4306(avg: 0.4305)\t Acc 0.794 (avg: 0.819)\n",
            "Epoch: [2][2/38]\t Loss 0.4465(avg: 0.4356)\t Acc 0.881 (avg: 0.839)\n",
            "Epoch: [2][3/38]\t Loss 0.5851(avg: 0.4734)\t Acc 0.794 (avg: 0.827)\n",
            "Epoch: [2][4/38]\t Loss 0.5152(avg: 0.4820)\t Acc 0.828 (avg: 0.827)\n",
            "Epoch: [2][5/38]\t Loss 0.5175(avg: 0.4878)\t Acc 0.839 (avg: 0.829)\n",
            "Epoch: [2][6/38]\t Loss 0.5761(avg: 0.5004)\t Acc 0.710 (avg: 0.812)\n",
            "Epoch: [2][7/38]\t Loss 0.6743(avg: 0.5226)\t Acc 0.703 (avg: 0.798)\n",
            "Epoch: [2][8/38]\t Loss 0.5199(avg: 0.5223)\t Acc 0.733 (avg: 0.791)\n",
            "Epoch: [2][9/38]\t Loss 0.5135(avg: 0.5214)\t Acc 0.767 (avg: 0.789)\n",
            "Epoch: [2][10/38]\t Loss 0.6735(avg: 0.5357)\t Acc 0.703 (avg: 0.781)\n",
            "Epoch: [2][11/38]\t Loss 0.5920(avg: 0.5404)\t Acc 0.746 (avg: 0.778)\n",
            "Epoch: [2][12/38]\t Loss 0.7832(avg: 0.5584)\t Acc 0.700 (avg: 0.772)\n",
            "Epoch: [2][13/38]\t Loss 0.4666(avg: 0.5519)\t Acc 0.839 (avg: 0.777)\n",
            "Epoch: [2][14/38]\t Loss 0.4876(avg: 0.5475)\t Acc 0.844 (avg: 0.782)\n",
            "Epoch: [2][15/38]\t Loss 0.5207(avg: 0.5459)\t Acc 0.783 (avg: 0.782)\n",
            "Epoch: [2][16/38]\t Loss 0.5479(avg: 0.5460)\t Acc 0.859 (avg: 0.786)\n",
            "Epoch: [2][17/38]\t Loss 0.5583(avg: 0.5467)\t Acc 0.823 (avg: 0.788)\n",
            "Epoch: [2][18/38]\t Loss 0.4660(avg: 0.5423)\t Acc 0.844 (avg: 0.791)\n",
            "Epoch: [2][19/38]\t Loss 0.4708(avg: 0.5387)\t Acc 0.794 (avg: 0.791)\n",
            "Epoch: [2][20/38]\t Loss 0.5443(avg: 0.5390)\t Acc 0.806 (avg: 0.792)\n",
            "Epoch: [2][21/38]\t Loss 0.3291(avg: 0.5295)\t Acc 0.887 (avg: 0.796)\n",
            "Epoch: [2][22/38]\t Loss 0.4849(avg: 0.5275)\t Acc 0.797 (avg: 0.797)\n",
            "Epoch: [2][23/38]\t Loss 0.4668(avg: 0.5250)\t Acc 0.839 (avg: 0.798)\n",
            "Epoch: [2][24/38]\t Loss 0.3729(avg: 0.5188)\t Acc 0.873 (avg: 0.801)\n",
            "Epoch: [2][25/38]\t Loss 0.4893(avg: 0.5177)\t Acc 0.778 (avg: 0.800)\n",
            "Epoch: [2][26/38]\t Loss 0.6329(avg: 0.5219)\t Acc 0.774 (avg: 0.799)\n",
            "Epoch: [2][27/38]\t Loss 0.4824(avg: 0.5205)\t Acc 0.839 (avg: 0.801)\n",
            "Epoch: [2][28/38]\t Loss 0.6065(avg: 0.5235)\t Acc 0.710 (avg: 0.798)\n",
            "Epoch: [2][29/38]\t Loss 0.3606(avg: 0.5180)\t Acc 0.857 (avg: 0.800)\n",
            "Epoch: [2][30/38]\t Loss 0.5229(avg: 0.5181)\t Acc 0.770 (avg: 0.799)\n",
            "Epoch: [2][31/38]\t Loss 0.4128(avg: 0.5148)\t Acc 0.778 (avg: 0.798)\n",
            "Epoch: [2][32/38]\t Loss 0.5062(avg: 0.5146)\t Acc 0.800 (avg: 0.798)\n",
            "Epoch: [2][33/38]\t Loss 0.4724(avg: 0.5133)\t Acc 0.825 (avg: 0.799)\n",
            "Epoch: [2][34/38]\t Loss 0.5953(avg: 0.5157)\t Acc 0.794 (avg: 0.799)\n",
            "Epoch: [2][35/38]\t Loss 0.5244(avg: 0.5159)\t Acc 0.803 (avg: 0.799)\n",
            "Epoch: [2][36/38]\t Loss 0.4000(avg: 0.5127)\t Acc 0.844 (avg: 0.800)\n",
            "Epoch: [2][37/38]\t Loss 0.5130(avg: 0.5127)\t Acc 0.800 (avg: 0.800)\n",
            "Epoch: [2]\t Avg Loss 0.5127\t Avg Accuracy 0.800\n",
            "Test Average Accuracy: 0.7137\n",
            "Epoch: [3][0/38]\t Loss 0.6773(avg: 0.6773)\t Acc 0.794 (avg: 0.794)\n",
            "Epoch: [3][1/38]\t Loss 0.5666(avg: 0.6243)\t Acc 0.776 (avg: 0.785)\n",
            "Epoch: [3][2/38]\t Loss 0.5515(avg: 0.5991)\t Acc 0.812 (avg: 0.795)\n",
            "Epoch: [3][3/38]\t Loss 0.5934(avg: 0.5977)\t Acc 0.730 (avg: 0.778)\n",
            "Epoch: [3][4/38]\t Loss 0.4465(avg: 0.5674)\t Acc 0.823 (avg: 0.787)\n",
            "Epoch: [3][5/38]\t Loss 0.4249(avg: 0.5437)\t Acc 0.839 (avg: 0.796)\n",
            "Epoch: [3][6/38]\t Loss 0.5623(avg: 0.5464)\t Acc 0.794 (avg: 0.795)\n",
            "Epoch: [3][7/38]\t Loss 0.3500(avg: 0.5222)\t Acc 0.902 (avg: 0.808)\n",
            "Epoch: [3][8/38]\t Loss 0.3923(avg: 0.5076)\t Acc 0.825 (avg: 0.810)\n",
            "Epoch: [3][9/38]\t Loss 0.4780(avg: 0.5046)\t Acc 0.810 (avg: 0.810)\n",
            "Epoch: [3][10/38]\t Loss 0.3801(avg: 0.4935)\t Acc 0.885 (avg: 0.817)\n",
            "Epoch: [3][11/38]\t Loss 0.5433(avg: 0.4976)\t Acc 0.754 (avg: 0.812)\n",
            "Epoch: [3][12/38]\t Loss 0.4245(avg: 0.4919)\t Acc 0.935 (avg: 0.821)\n",
            "Epoch: [3][13/38]\t Loss 0.4038(avg: 0.4855)\t Acc 0.825 (avg: 0.822)\n",
            "Epoch: [3][14/38]\t Loss 0.4409(avg: 0.4825)\t Acc 0.844 (avg: 0.823)\n",
            "Epoch: [3][15/38]\t Loss 0.3227(avg: 0.4724)\t Acc 0.857 (avg: 0.825)\n",
            "Epoch: [3][16/38]\t Loss 0.3663(avg: 0.4663)\t Acc 0.885 (avg: 0.829)\n",
            "Epoch: [3][17/38]\t Loss 0.4743(avg: 0.4667)\t Acc 0.803 (avg: 0.827)\n",
            "Epoch: [3][18/38]\t Loss 0.4135(avg: 0.4638)\t Acc 0.812 (avg: 0.827)\n",
            "Epoch: [3][19/38]\t Loss 0.3640(avg: 0.4589)\t Acc 0.869 (avg: 0.829)\n",
            "Epoch: [3][20/38]\t Loss 0.3656(avg: 0.4546)\t Acc 0.867 (avg: 0.830)\n",
            "Epoch: [3][21/38]\t Loss 0.5703(avg: 0.4599)\t Acc 0.774 (avg: 0.828)\n",
            "Epoch: [3][22/38]\t Loss 0.4225(avg: 0.4582)\t Acc 0.812 (avg: 0.827)\n",
            "Epoch: [3][23/38]\t Loss 0.6429(avg: 0.4659)\t Acc 0.774 (avg: 0.825)\n",
            "Epoch: [3][24/38]\t Loss 0.5185(avg: 0.4680)\t Acc 0.810 (avg: 0.824)\n",
            "Epoch: [3][25/38]\t Loss 0.4919(avg: 0.4689)\t Acc 0.852 (avg: 0.825)\n",
            "Epoch: [3][26/38]\t Loss 0.2400(avg: 0.4602)\t Acc 0.922 (avg: 0.829)\n",
            "Epoch: [3][27/38]\t Loss 0.4270(avg: 0.4590)\t Acc 0.828 (avg: 0.829)\n",
            "Epoch: [3][28/38]\t Loss 0.5620(avg: 0.4626)\t Acc 0.828 (avg: 0.829)\n",
            "Epoch: [3][29/38]\t Loss 0.3296(avg: 0.4583)\t Acc 0.852 (avg: 0.830)\n",
            "Epoch: [3][30/38]\t Loss 0.5196(avg: 0.4603)\t Acc 0.762 (avg: 0.828)\n",
            "Epoch: [3][31/38]\t Loss 0.4456(avg: 0.4598)\t Acc 0.810 (avg: 0.827)\n",
            "Epoch: [3][32/38]\t Loss 0.3940(avg: 0.4578)\t Acc 0.873 (avg: 0.828)\n",
            "Epoch: [3][33/38]\t Loss 0.6183(avg: 0.4625)\t Acc 0.742 (avg: 0.826)\n",
            "Epoch: [3][34/38]\t Loss 0.4005(avg: 0.4607)\t Acc 0.859 (avg: 0.827)\n",
            "Epoch: [3][35/38]\t Loss 0.5175(avg: 0.4622)\t Acc 0.823 (avg: 0.827)\n",
            "Epoch: [3][36/38]\t Loss 0.4209(avg: 0.4611)\t Acc 0.823 (avg: 0.827)\n",
            "Epoch: [3][37/38]\t Loss 0.2915(avg: 0.4608)\t Acc 0.800 (avg: 0.827)\n",
            "Epoch: [3]\t Avg Loss 0.4608\t Avg Accuracy 0.827\n",
            "Test Average Accuracy: 0.7768\n",
            "Saving Model...\n",
            "Epoch: [4][0/38]\t Loss 0.3968(avg: 0.3968)\t Acc 0.817 (avg: 0.817)\n",
            "Epoch: [4][1/38]\t Loss 0.4022(avg: 0.3995)\t Acc 0.839 (avg: 0.828)\n",
            "Epoch: [4][2/38]\t Loss 0.4717(avg: 0.4233)\t Acc 0.783 (avg: 0.813)\n",
            "Epoch: [4][3/38]\t Loss 0.3839(avg: 0.4134)\t Acc 0.902 (avg: 0.835)\n",
            "Epoch: [4][4/38]\t Loss 0.3529(avg: 0.4010)\t Acc 0.889 (avg: 0.846)\n",
            "Epoch: [4][5/38]\t Loss 0.5292(avg: 0.4229)\t Acc 0.794 (avg: 0.837)\n",
            "Epoch: [4][6/38]\t Loss 0.4402(avg: 0.4254)\t Acc 0.828 (avg: 0.836)\n",
            "Epoch: [4][7/38]\t Loss 0.2926(avg: 0.4088)\t Acc 0.887 (avg: 0.842)\n",
            "Epoch: [4][8/38]\t Loss 0.3525(avg: 0.4024)\t Acc 0.825 (avg: 0.841)\n",
            "Epoch: [4][9/38]\t Loss 0.3588(avg: 0.3981)\t Acc 0.903 (avg: 0.847)\n",
            "Epoch: [4][10/38]\t Loss 0.2841(avg: 0.3875)\t Acc 0.889 (avg: 0.851)\n",
            "Epoch: [4][11/38]\t Loss 0.3628(avg: 0.3855)\t Acc 0.857 (avg: 0.851)\n",
            "Epoch: [4][12/38]\t Loss 0.4094(avg: 0.3873)\t Acc 0.825 (avg: 0.849)\n",
            "Epoch: [4][13/38]\t Loss 0.4300(avg: 0.3904)\t Acc 0.810 (avg: 0.846)\n",
            "Epoch: [4][14/38]\t Loss 0.3492(avg: 0.3877)\t Acc 0.839 (avg: 0.846)\n",
            "Epoch: [4][15/38]\t Loss 0.4197(avg: 0.3897)\t Acc 0.857 (avg: 0.847)\n",
            "Epoch: [4][16/38]\t Loss 0.2760(avg: 0.3828)\t Acc 0.922 (avg: 0.851)\n",
            "Epoch: [4][17/38]\t Loss 0.3383(avg: 0.3803)\t Acc 0.844 (avg: 0.851)\n",
            "Epoch: [4][18/38]\t Loss 0.2891(avg: 0.3756)\t Acc 0.902 (avg: 0.853)\n",
            "Epoch: [4][19/38]\t Loss 0.2935(avg: 0.3717)\t Acc 0.883 (avg: 0.855)\n",
            "Epoch: [4][20/38]\t Loss 0.6434(avg: 0.3843)\t Acc 0.820 (avg: 0.853)\n",
            "Epoch: [4][21/38]\t Loss 0.3837(avg: 0.3843)\t Acc 0.828 (avg: 0.852)\n",
            "Epoch: [4][22/38]\t Loss 0.3583(avg: 0.3832)\t Acc 0.836 (avg: 0.851)\n",
            "Epoch: [4][23/38]\t Loss 0.4924(avg: 0.3879)\t Acc 0.797 (avg: 0.849)\n",
            "Epoch: [4][24/38]\t Loss 0.2640(avg: 0.3829)\t Acc 0.919 (avg: 0.852)\n",
            "Epoch: [4][25/38]\t Loss 0.4661(avg: 0.3862)\t Acc 0.794 (avg: 0.849)\n",
            "Epoch: [4][26/38]\t Loss 0.3356(avg: 0.3843)\t Acc 0.873 (avg: 0.850)\n",
            "Epoch: [4][27/38]\t Loss 0.3178(avg: 0.3820)\t Acc 0.869 (avg: 0.851)\n",
            "Epoch: [4][28/38]\t Loss 0.3803(avg: 0.3819)\t Acc 0.869 (avg: 0.852)\n",
            "Epoch: [4][29/38]\t Loss 0.4604(avg: 0.3846)\t Acc 0.844 (avg: 0.851)\n",
            "Epoch: [4][30/38]\t Loss 0.4159(avg: 0.3856)\t Acc 0.823 (avg: 0.850)\n",
            "Epoch: [4][31/38]\t Loss 0.4807(avg: 0.3886)\t Acc 0.828 (avg: 0.850)\n",
            "Epoch: [4][32/38]\t Loss 0.3390(avg: 0.3872)\t Acc 0.871 (avg: 0.850)\n",
            "Epoch: [4][33/38]\t Loss 0.2914(avg: 0.3845)\t Acc 0.932 (avg: 0.853)\n",
            "Epoch: [4][34/38]\t Loss 0.4367(avg: 0.3860)\t Acc 0.825 (avg: 0.852)\n",
            "Epoch: [4][35/38]\t Loss 0.2552(avg: 0.3823)\t Acc 0.952 (avg: 0.855)\n",
            "Epoch: [4][36/38]\t Loss 0.4601(avg: 0.3845)\t Acc 0.812 (avg: 0.853)\n",
            "Epoch: [4][37/38]\t Loss 0.0022(avg: 0.3837)\t Acc 1.000 (avg: 0.854)\n",
            "Epoch: [4]\t Avg Loss 0.3837\t Avg Accuracy 0.854\n",
            "Test Average Accuracy: 0.7768\n",
            "Saving Model...\n",
            "Epoch: [5][0/38]\t Loss 0.2518(avg: 0.2518)\t Acc 0.875 (avg: 0.875)\n",
            "Epoch: [5][1/38]\t Loss 0.3849(avg: 0.3178)\t Acc 0.921 (avg: 0.898)\n",
            "Epoch: [5][2/38]\t Loss 0.3944(avg: 0.3432)\t Acc 0.873 (avg: 0.889)\n",
            "Epoch: [5][3/38]\t Loss 0.3653(avg: 0.3488)\t Acc 0.828 (avg: 0.874)\n",
            "Epoch: [5][4/38]\t Loss 0.2400(avg: 0.3269)\t Acc 0.922 (avg: 0.884)\n",
            "Epoch: [5][5/38]\t Loss 0.4771(avg: 0.3514)\t Acc 0.790 (avg: 0.868)\n",
            "Epoch: [5][6/38]\t Loss 0.3954(avg: 0.3577)\t Acc 0.844 (avg: 0.865)\n",
            "Epoch: [5][7/38]\t Loss 0.4378(avg: 0.3677)\t Acc 0.857 (avg: 0.864)\n",
            "Epoch: [5][8/38]\t Loss 0.3481(avg: 0.3655)\t Acc 0.875 (avg: 0.865)\n",
            "Epoch: [5][9/38]\t Loss 0.2556(avg: 0.3547)\t Acc 0.903 (avg: 0.869)\n",
            "Epoch: [5][10/38]\t Loss 0.3762(avg: 0.3567)\t Acc 0.873 (avg: 0.869)\n",
            "Epoch: [5][11/38]\t Loss 0.3192(avg: 0.3537)\t Acc 0.867 (avg: 0.869)\n",
            "Epoch: [5][12/38]\t Loss 0.2891(avg: 0.3487)\t Acc 0.891 (avg: 0.871)\n",
            "Epoch: [5][13/38]\t Loss 0.3298(avg: 0.3473)\t Acc 0.855 (avg: 0.870)\n",
            "Epoch: [5][14/38]\t Loss 0.2696(avg: 0.3422)\t Acc 0.921 (avg: 0.873)\n",
            "Epoch: [5][15/38]\t Loss 0.2829(avg: 0.3388)\t Acc 0.911 (avg: 0.875)\n",
            "Epoch: [5][16/38]\t Loss 0.3796(avg: 0.3412)\t Acc 0.839 (avg: 0.873)\n",
            "Epoch: [5][17/38]\t Loss 0.2852(avg: 0.3380)\t Acc 0.922 (avg: 0.876)\n",
            "Epoch: [5][18/38]\t Loss 0.3161(avg: 0.3369)\t Acc 0.855 (avg: 0.875)\n",
            "Epoch: [5][19/38]\t Loss 0.5431(avg: 0.3471)\t Acc 0.823 (avg: 0.872)\n",
            "Epoch: [5][20/38]\t Loss 0.2846(avg: 0.3441)\t Acc 0.875 (avg: 0.872)\n",
            "Epoch: [5][21/38]\t Loss 0.3552(avg: 0.3445)\t Acc 0.850 (avg: 0.871)\n",
            "Epoch: [5][22/38]\t Loss 0.2840(avg: 0.3419)\t Acc 0.875 (avg: 0.871)\n",
            "Epoch: [5][23/38]\t Loss 0.2707(avg: 0.3389)\t Acc 0.857 (avg: 0.871)\n",
            "Epoch: [5][24/38]\t Loss 0.3395(avg: 0.3389)\t Acc 0.889 (avg: 0.872)\n",
            "Epoch: [5][25/38]\t Loss 0.3751(avg: 0.3403)\t Acc 0.841 (avg: 0.870)\n",
            "Epoch: [5][26/38]\t Loss 0.3074(avg: 0.3391)\t Acc 0.889 (avg: 0.871)\n",
            "Epoch: [5][27/38]\t Loss 0.3846(avg: 0.3407)\t Acc 0.869 (avg: 0.871)\n",
            "Epoch: [5][28/38]\t Loss 0.3271(avg: 0.3402)\t Acc 0.887 (avg: 0.872)\n",
            "Epoch: [5][29/38]\t Loss 0.2387(avg: 0.3368)\t Acc 0.887 (avg: 0.872)\n",
            "Epoch: [5][30/38]\t Loss 0.3983(avg: 0.3387)\t Acc 0.831 (avg: 0.871)\n",
            "Epoch: [5][31/38]\t Loss 0.4088(avg: 0.3409)\t Acc 0.889 (avg: 0.871)\n",
            "Epoch: [5][32/38]\t Loss 0.2759(avg: 0.3389)\t Acc 0.937 (avg: 0.873)\n",
            "Epoch: [5][33/38]\t Loss 0.2782(avg: 0.3371)\t Acc 0.905 (avg: 0.874)\n",
            "Epoch: [5][34/38]\t Loss 0.2385(avg: 0.3344)\t Acc 0.934 (avg: 0.876)\n",
            "Epoch: [5][35/38]\t Loss 0.3684(avg: 0.3353)\t Acc 0.900 (avg: 0.877)\n",
            "Epoch: [5][36/38]\t Loss 0.3729(avg: 0.3363)\t Acc 0.871 (avg: 0.876)\n",
            "Epoch: [5][37/38]\t Loss 0.2899(avg: 0.3362)\t Acc 0.800 (avg: 0.876)\n",
            "Epoch: [5]\t Avg Loss 0.3362\t Avg Accuracy 0.876\n",
            "Test Average Accuracy: 0.7728\n",
            "Epoch: [6][0/38]\t Loss 0.1882(avg: 0.1882)\t Acc 0.938 (avg: 0.938)\n",
            "Epoch: [6][1/38]\t Loss 0.2332(avg: 0.2107)\t Acc 0.906 (avg: 0.922)\n",
            "Epoch: [6][2/38]\t Loss 0.2950(avg: 0.2388)\t Acc 0.938 (avg: 0.927)\n",
            "Epoch: [6][3/38]\t Loss 0.2763(avg: 0.2477)\t Acc 0.867 (avg: 0.913)\n",
            "Epoch: [6][4/38]\t Loss 0.3279(avg: 0.2638)\t Acc 0.905 (avg: 0.911)\n",
            "Epoch: [6][5/38]\t Loss 0.2027(avg: 0.2536)\t Acc 0.921 (avg: 0.913)\n",
            "Epoch: [6][6/38]\t Loss 0.3014(avg: 0.2603)\t Acc 0.887 (avg: 0.909)\n",
            "Epoch: [6][7/38]\t Loss 0.3541(avg: 0.2721)\t Acc 0.873 (avg: 0.905)\n",
            "Epoch: [6][8/38]\t Loss 0.2533(avg: 0.2700)\t Acc 0.918 (avg: 0.906)\n",
            "Epoch: [6][9/38]\t Loss 0.2055(avg: 0.2637)\t Acc 0.934 (avg: 0.909)\n",
            "Epoch: [6][10/38]\t Loss 0.2424(avg: 0.2619)\t Acc 0.900 (avg: 0.908)\n",
            "Epoch: [6][11/38]\t Loss 0.2437(avg: 0.2604)\t Acc 0.919 (avg: 0.909)\n",
            "Epoch: [6][12/38]\t Loss 0.4023(avg: 0.2716)\t Acc 0.812 (avg: 0.901)\n",
            "Epoch: [6][13/38]\t Loss 0.2392(avg: 0.2692)\t Acc 0.873 (avg: 0.899)\n",
            "Epoch: [6][14/38]\t Loss 0.3536(avg: 0.2747)\t Acc 0.885 (avg: 0.898)\n",
            "Epoch: [6][15/38]\t Loss 0.1988(avg: 0.2699)\t Acc 0.922 (avg: 0.900)\n",
            "Epoch: [6][16/38]\t Loss 0.1537(avg: 0.2631)\t Acc 0.952 (avg: 0.903)\n",
            "Epoch: [6][17/38]\t Loss 0.2392(avg: 0.2617)\t Acc 0.891 (avg: 0.902)\n",
            "Epoch: [6][18/38]\t Loss 0.3243(avg: 0.2650)\t Acc 0.889 (avg: 0.902)\n",
            "Epoch: [6][19/38]\t Loss 0.1830(avg: 0.2612)\t Acc 0.949 (avg: 0.904)\n",
            "Epoch: [6][20/38]\t Loss 0.3640(avg: 0.2660)\t Acc 0.887 (avg: 0.903)\n",
            "Epoch: [6][21/38]\t Loss 0.3833(avg: 0.2715)\t Acc 0.844 (avg: 0.900)\n",
            "Epoch: [6][22/38]\t Loss 0.2838(avg: 0.2720)\t Acc 0.869 (avg: 0.899)\n",
            "Epoch: [6][23/38]\t Loss 0.2389(avg: 0.2707)\t Acc 0.918 (avg: 0.900)\n",
            "Epoch: [6][24/38]\t Loss 0.1142(avg: 0.2644)\t Acc 0.952 (avg: 0.902)\n",
            "Epoch: [6][25/38]\t Loss 0.4476(avg: 0.2713)\t Acc 0.836 (avg: 0.899)\n",
            "Epoch: [6][26/38]\t Loss 0.5092(avg: 0.2801)\t Acc 0.790 (avg: 0.895)\n",
            "Epoch: [6][27/38]\t Loss 0.5006(avg: 0.2882)\t Acc 0.828 (avg: 0.893)\n",
            "Epoch: [6][28/38]\t Loss 0.2374(avg: 0.2865)\t Acc 0.919 (avg: 0.894)\n",
            "Epoch: [6][29/38]\t Loss 0.3941(avg: 0.2900)\t Acc 0.871 (avg: 0.893)\n",
            "Epoch: [6][30/38]\t Loss 0.3887(avg: 0.2932)\t Acc 0.871 (avg: 0.892)\n",
            "Epoch: [6][31/38]\t Loss 0.2399(avg: 0.2915)\t Acc 0.891 (avg: 0.892)\n",
            "Epoch: [6][32/38]\t Loss 0.6163(avg: 0.3016)\t Acc 0.812 (avg: 0.890)\n",
            "Epoch: [6][33/38]\t Loss 0.3908(avg: 0.3042)\t Acc 0.823 (avg: 0.888)\n",
            "Epoch: [6][34/38]\t Loss 0.2090(avg: 0.3015)\t Acc 0.918 (avg: 0.889)\n",
            "Epoch: [6][35/38]\t Loss 0.2638(avg: 0.3005)\t Acc 0.875 (avg: 0.888)\n",
            "Epoch: [6][36/38]\t Loss 0.2923(avg: 0.3002)\t Acc 0.887 (avg: 0.888)\n",
            "Epoch: [6][37/38]\t Loss 0.1906(avg: 0.3000)\t Acc 1.000 (avg: 0.888)\n",
            "Epoch: [6]\t Avg Loss 0.3000\t Avg Accuracy 0.888\n",
            "Test Average Accuracy: 0.7886\n",
            "Saving Model...\n",
            "Epoch: [7][0/38]\t Loss 0.1281(avg: 0.1281)\t Acc 1.000 (avg: 1.000)\n",
            "Epoch: [7][1/38]\t Loss 0.3190(avg: 0.2235)\t Acc 0.841 (avg: 0.921)\n",
            "Epoch: [7][2/38]\t Loss 0.3763(avg: 0.2734)\t Acc 0.869 (avg: 0.904)\n",
            "Epoch: [7][3/38]\t Loss 0.2161(avg: 0.2591)\t Acc 0.903 (avg: 0.904)\n",
            "Epoch: [7][4/38]\t Loss 0.1960(avg: 0.2464)\t Acc 0.921 (avg: 0.907)\n",
            "Epoch: [7][5/38]\t Loss 0.0873(avg: 0.2196)\t Acc 0.968 (avg: 0.917)\n",
            "Epoch: [7][6/38]\t Loss 0.1869(avg: 0.2149)\t Acc 0.937 (avg: 0.920)\n",
            "Epoch: [7][7/38]\t Loss 0.2097(avg: 0.2143)\t Acc 0.951 (avg: 0.924)\n",
            "Epoch: [7][8/38]\t Loss 0.3946(avg: 0.2345)\t Acc 0.873 (avg: 0.918)\n",
            "Epoch: [7][9/38]\t Loss 0.3076(avg: 0.2418)\t Acc 0.887 (avg: 0.915)\n",
            "Epoch: [7][10/38]\t Loss 0.2274(avg: 0.2405)\t Acc 0.919 (avg: 0.915)\n",
            "Epoch: [7][11/38]\t Loss 0.2507(avg: 0.2413)\t Acc 0.891 (avg: 0.913)\n",
            "Epoch: [7][12/38]\t Loss 0.2612(avg: 0.2429)\t Acc 0.873 (avg: 0.910)\n",
            "Epoch: [7][13/38]\t Loss 0.0925(avg: 0.2321)\t Acc 0.968 (avg: 0.914)\n",
            "Epoch: [7][14/38]\t Loss 0.2404(avg: 0.2326)\t Acc 0.898 (avg: 0.913)\n",
            "Epoch: [7][15/38]\t Loss 0.3182(avg: 0.2379)\t Acc 0.871 (avg: 0.911)\n",
            "Epoch: [7][16/38]\t Loss 0.3747(avg: 0.2459)\t Acc 0.919 (avg: 0.911)\n",
            "Epoch: [7][17/38]\t Loss 0.1763(avg: 0.2421)\t Acc 0.935 (avg: 0.913)\n",
            "Epoch: [7][18/38]\t Loss 0.4806(avg: 0.2542)\t Acc 0.833 (avg: 0.909)\n",
            "Epoch: [7][19/38]\t Loss 0.2504(avg: 0.2540)\t Acc 0.889 (avg: 0.908)\n",
            "Epoch: [7][20/38]\t Loss 0.2458(avg: 0.2536)\t Acc 0.921 (avg: 0.908)\n",
            "Epoch: [7][21/38]\t Loss 0.1562(avg: 0.2492)\t Acc 0.968 (avg: 0.911)\n",
            "Epoch: [7][22/38]\t Loss 0.2995(avg: 0.2514)\t Acc 0.889 (avg: 0.910)\n",
            "Epoch: [7][23/38]\t Loss 0.2560(avg: 0.2516)\t Acc 0.935 (avg: 0.911)\n",
            "Epoch: [7][24/38]\t Loss 0.4507(avg: 0.2594)\t Acc 0.852 (avg: 0.909)\n",
            "Epoch: [7][25/38]\t Loss 0.4091(avg: 0.2651)\t Acc 0.869 (avg: 0.907)\n",
            "Epoch: [7][26/38]\t Loss 0.4905(avg: 0.2735)\t Acc 0.810 (avg: 0.904)\n",
            "Epoch: [7][27/38]\t Loss 0.2961(avg: 0.2743)\t Acc 0.857 (avg: 0.902)\n",
            "Epoch: [7][28/38]\t Loss 0.2409(avg: 0.2732)\t Acc 0.935 (avg: 0.903)\n",
            "Epoch: [7][29/38]\t Loss 0.3120(avg: 0.2745)\t Acc 0.875 (avg: 0.902)\n",
            "Epoch: [7][30/38]\t Loss 0.2082(avg: 0.2723)\t Acc 0.889 (avg: 0.902)\n",
            "Epoch: [7][31/38]\t Loss 0.3317(avg: 0.2742)\t Acc 0.839 (avg: 0.900)\n",
            "Epoch: [7][32/38]\t Loss 0.3697(avg: 0.2772)\t Acc 0.844 (avg: 0.898)\n",
            "Epoch: [7][33/38]\t Loss 0.2571(avg: 0.2766)\t Acc 0.937 (avg: 0.899)\n",
            "Epoch: [7][34/38]\t Loss 0.3311(avg: 0.2781)\t Acc 0.905 (avg: 0.899)\n",
            "Epoch: [7][35/38]\t Loss 0.3129(avg: 0.2791)\t Acc 0.871 (avg: 0.898)\n",
            "Epoch: [7][36/38]\t Loss 0.1743(avg: 0.2762)\t Acc 0.921 (avg: 0.899)\n",
            "Epoch: [7][37/38]\t Loss 0.0388(avg: 0.2758)\t Acc 1.000 (avg: 0.899)\n",
            "Epoch: [7]\t Avg Loss 0.2758\t Avg Accuracy 0.899\n",
            "Test Average Accuracy: 0.7695\n",
            "Epoch: [8][0/38]\t Loss 0.3074(avg: 0.3074)\t Acc 0.859 (avg: 0.859)\n",
            "Epoch: [8][1/38]\t Loss 0.2036(avg: 0.2555)\t Acc 0.891 (avg: 0.875)\n",
            "Epoch: [8][2/38]\t Loss 0.1952(avg: 0.2358)\t Acc 0.919 (avg: 0.889)\n",
            "Epoch: [8][3/38]\t Loss 0.1074(avg: 0.2050)\t Acc 0.983 (avg: 0.912)\n",
            "Epoch: [8][4/38]\t Loss 0.1591(avg: 0.1959)\t Acc 0.935 (avg: 0.917)\n",
            "Epoch: [8][5/38]\t Loss 0.1331(avg: 0.1852)\t Acc 0.969 (avg: 0.926)\n",
            "Epoch: [8][6/38]\t Loss 0.2154(avg: 0.1895)\t Acc 0.889 (avg: 0.920)\n",
            "Epoch: [8][7/38]\t Loss 0.0771(avg: 0.1758)\t Acc 1.000 (avg: 0.930)\n",
            "Epoch: [8][8/38]\t Loss 0.1135(avg: 0.1689)\t Acc 0.968 (avg: 0.934)\n",
            "Epoch: [8][9/38]\t Loss 0.2830(avg: 0.1803)\t Acc 0.887 (avg: 0.929)\n",
            "Epoch: [8][10/38]\t Loss 0.2684(avg: 0.1881)\t Acc 0.852 (avg: 0.923)\n",
            "Epoch: [8][11/38]\t Loss 0.2735(avg: 0.1954)\t Acc 0.859 (avg: 0.917)\n",
            "Epoch: [8][12/38]\t Loss 0.1653(avg: 0.1931)\t Acc 0.905 (avg: 0.916)\n",
            "Epoch: [8][13/38]\t Loss 0.2886(avg: 0.1998)\t Acc 0.903 (avg: 0.915)\n",
            "Epoch: [8][14/38]\t Loss 0.3027(avg: 0.2065)\t Acc 0.883 (avg: 0.913)\n",
            "Epoch: [8][15/38]\t Loss 0.2044(avg: 0.2063)\t Acc 0.952 (avg: 0.916)\n",
            "Epoch: [8][16/38]\t Loss 0.2003(avg: 0.2060)\t Acc 0.937 (avg: 0.917)\n",
            "Epoch: [8][17/38]\t Loss 0.2736(avg: 0.2098)\t Acc 0.873 (avg: 0.914)\n",
            "Epoch: [8][18/38]\t Loss 0.1259(avg: 0.2053)\t Acc 0.952 (avg: 0.916)\n",
            "Epoch: [8][19/38]\t Loss 0.2463(avg: 0.2073)\t Acc 0.887 (avg: 0.915)\n",
            "Epoch: [8][20/38]\t Loss 0.3520(avg: 0.2144)\t Acc 0.906 (avg: 0.915)\n",
            "Epoch: [8][21/38]\t Loss 0.1881(avg: 0.2132)\t Acc 0.905 (avg: 0.914)\n",
            "Epoch: [8][22/38]\t Loss 0.3392(avg: 0.2186)\t Acc 0.887 (avg: 0.913)\n",
            "Epoch: [8][23/38]\t Loss 0.1746(avg: 0.2168)\t Acc 0.934 (avg: 0.914)\n",
            "Epoch: [8][24/38]\t Loss 0.3198(avg: 0.2210)\t Acc 0.905 (avg: 0.913)\n",
            "Epoch: [8][25/38]\t Loss 0.5442(avg: 0.2334)\t Acc 0.839 (avg: 0.911)\n",
            "Epoch: [8][26/38]\t Loss 0.2551(avg: 0.2342)\t Acc 0.905 (avg: 0.910)\n",
            "Epoch: [8][27/38]\t Loss 0.1142(avg: 0.2298)\t Acc 0.969 (avg: 0.913)\n",
            "Epoch: [8][28/38]\t Loss 0.2972(avg: 0.2321)\t Acc 0.887 (avg: 0.912)\n",
            "Epoch: [8][29/38]\t Loss 0.2823(avg: 0.2338)\t Acc 0.889 (avg: 0.911)\n",
            "Epoch: [8][30/38]\t Loss 0.2111(avg: 0.2331)\t Acc 0.918 (avg: 0.911)\n",
            "Epoch: [8][31/38]\t Loss 0.1776(avg: 0.2314)\t Acc 0.951 (avg: 0.912)\n",
            "Epoch: [8][32/38]\t Loss 0.2794(avg: 0.2329)\t Acc 0.891 (avg: 0.912)\n",
            "Epoch: [8][33/38]\t Loss 0.3644(avg: 0.2367)\t Acc 0.855 (avg: 0.910)\n",
            "Epoch: [8][34/38]\t Loss 0.2270(avg: 0.2364)\t Acc 0.889 (avg: 0.909)\n",
            "Epoch: [8][35/38]\t Loss 0.2237(avg: 0.2361)\t Acc 0.900 (avg: 0.909)\n",
            "Epoch: [8][36/38]\t Loss 0.3419(avg: 0.2390)\t Acc 0.825 (avg: 0.907)\n",
            "Epoch: [8][37/38]\t Loss 0.0059(avg: 0.2386)\t Acc 1.000 (avg: 0.907)\n",
            "Epoch: [8]\t Avg Loss 0.2386\t Avg Accuracy 0.907\n",
            "Test Average Accuracy: 0.7814\n",
            "Epoch: [9][0/38]\t Loss 0.3098(avg: 0.3098)\t Acc 0.903 (avg: 0.903)\n",
            "Epoch: [9][1/38]\t Loss 0.1474(avg: 0.2273)\t Acc 0.953 (avg: 0.929)\n",
            "Epoch: [9][2/38]\t Loss 0.3044(avg: 0.2533)\t Acc 0.891 (avg: 0.916)\n",
            "Epoch: [9][3/38]\t Loss 0.1495(avg: 0.2281)\t Acc 0.951 (avg: 0.924)\n",
            "Epoch: [9][4/38]\t Loss 0.2457(avg: 0.2316)\t Acc 0.873 (avg: 0.914)\n",
            "Epoch: [9][5/38]\t Loss 0.2018(avg: 0.2266)\t Acc 0.937 (avg: 0.918)\n",
            "Epoch: [9][6/38]\t Loss 0.1488(avg: 0.2155)\t Acc 0.952 (avg: 0.923)\n",
            "Epoch: [9][7/38]\t Loss 0.1297(avg: 0.2049)\t Acc 0.984 (avg: 0.930)\n",
            "Epoch: [9][8/38]\t Loss 0.1122(avg: 0.1950)\t Acc 0.983 (avg: 0.936)\n",
            "Epoch: [9][9/38]\t Loss 0.1906(avg: 0.1945)\t Acc 0.937 (avg: 0.936)\n",
            "Epoch: [9][10/38]\t Loss 0.1401(avg: 0.1896)\t Acc 0.952 (avg: 0.938)\n",
            "Epoch: [9][11/38]\t Loss 0.1464(avg: 0.1860)\t Acc 0.951 (avg: 0.939)\n",
            "Epoch: [9][12/38]\t Loss 0.1472(avg: 0.1830)\t Acc 0.953 (avg: 0.940)\n",
            "Epoch: [9][13/38]\t Loss 0.1611(avg: 0.1814)\t Acc 0.938 (avg: 0.940)\n",
            "Epoch: [9][14/38]\t Loss 0.2548(avg: 0.1862)\t Acc 0.918 (avg: 0.938)\n",
            "Epoch: [9][15/38]\t Loss 0.1852(avg: 0.1861)\t Acc 0.919 (avg: 0.937)\n",
            "Epoch: [9][16/38]\t Loss 0.4812(avg: 0.2033)\t Acc 0.806 (avg: 0.929)\n",
            "Epoch: [9][17/38]\t Loss 0.3968(avg: 0.2143)\t Acc 0.859 (avg: 0.925)\n",
            "Epoch: [9][18/38]\t Loss 0.2246(avg: 0.2148)\t Acc 0.933 (avg: 0.926)\n",
            "Epoch: [9][19/38]\t Loss 0.0920(avg: 0.2086)\t Acc 0.984 (avg: 0.929)\n",
            "Epoch: [9][20/38]\t Loss 0.0971(avg: 0.2035)\t Acc 0.984 (avg: 0.931)\n",
            "Epoch: [9][21/38]\t Loss 0.3044(avg: 0.2080)\t Acc 0.871 (avg: 0.929)\n",
            "Epoch: [9][22/38]\t Loss 0.2799(avg: 0.2111)\t Acc 0.919 (avg: 0.928)\n",
            "Epoch: [9][23/38]\t Loss 0.2511(avg: 0.2128)\t Acc 0.889 (avg: 0.927)\n",
            "Epoch: [9][24/38]\t Loss 0.2628(avg: 0.2148)\t Acc 0.921 (avg: 0.926)\n",
            "Epoch: [9][25/38]\t Loss 0.2997(avg: 0.2181)\t Acc 0.905 (avg: 0.925)\n",
            "Epoch: [9][26/38]\t Loss 0.0835(avg: 0.2132)\t Acc 0.984 (avg: 0.928)\n",
            "Epoch: [9][27/38]\t Loss 0.3157(avg: 0.2169)\t Acc 0.857 (avg: 0.925)\n",
            "Epoch: [9][28/38]\t Loss 0.2367(avg: 0.2176)\t Acc 0.922 (avg: 0.925)\n",
            "Epoch: [9][29/38]\t Loss 0.1937(avg: 0.2168)\t Acc 0.933 (avg: 0.925)\n",
            "Epoch: [9][30/38]\t Loss 0.1534(avg: 0.2147)\t Acc 0.952 (avg: 0.926)\n",
            "Epoch: [9][31/38]\t Loss 0.1982(avg: 0.2142)\t Acc 0.903 (avg: 0.925)\n",
            "Epoch: [9][32/38]\t Loss 0.1149(avg: 0.2113)\t Acc 0.950 (avg: 0.926)\n",
            "Epoch: [9][33/38]\t Loss 0.2066(avg: 0.2112)\t Acc 0.905 (avg: 0.925)\n",
            "Epoch: [9][34/38]\t Loss 0.0866(avg: 0.2077)\t Acc 0.967 (avg: 0.927)\n",
            "Epoch: [9][35/38]\t Loss 0.1658(avg: 0.2065)\t Acc 0.937 (avg: 0.927)\n",
            "Epoch: [9][36/38]\t Loss 0.2376(avg: 0.2074)\t Acc 0.905 (avg: 0.926)\n",
            "Epoch: [9][37/38]\t Loss 0.2686(avg: 0.2075)\t Acc 1.000 (avg: 0.926)\n",
            "Epoch: [9]\t Avg Loss 0.2075\t Avg Accuracy 0.926\n",
            "Test Average Accuracy: 0.7873\n",
            "Epoch: [10][0/38]\t Loss 0.0629(avg: 0.0629)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [10][1/38]\t Loss 0.1918(avg: 0.1269)\t Acc 0.952 (avg: 0.969)\n",
            "Epoch: [10][2/38]\t Loss 0.1391(avg: 0.1309)\t Acc 0.935 (avg: 0.958)\n",
            "Epoch: [10][3/38]\t Loss 0.1231(avg: 0.1290)\t Acc 0.952 (avg: 0.956)\n",
            "Epoch: [10][4/38]\t Loss 0.2038(avg: 0.1438)\t Acc 0.952 (avg: 0.955)\n",
            "Epoch: [10][5/38]\t Loss 0.1761(avg: 0.1493)\t Acc 0.938 (avg: 0.952)\n",
            "Epoch: [10][6/38]\t Loss 0.1252(avg: 0.1459)\t Acc 0.968 (avg: 0.954)\n",
            "Epoch: [10][7/38]\t Loss 0.2000(avg: 0.1526)\t Acc 0.935 (avg: 0.952)\n",
            "Epoch: [10][8/38]\t Loss 0.1363(avg: 0.1508)\t Acc 0.952 (avg: 0.952)\n",
            "Epoch: [10][9/38]\t Loss 0.1147(avg: 0.1472)\t Acc 0.952 (avg: 0.952)\n",
            "Epoch: [10][10/38]\t Loss 0.1841(avg: 0.1505)\t Acc 0.935 (avg: 0.951)\n",
            "Epoch: [10][11/38]\t Loss 0.1742(avg: 0.1525)\t Acc 0.935 (avg: 0.949)\n",
            "Epoch: [10][12/38]\t Loss 0.1727(avg: 0.1540)\t Acc 0.935 (avg: 0.948)\n",
            "Epoch: [10][13/38]\t Loss 0.2200(avg: 0.1587)\t Acc 0.952 (avg: 0.949)\n",
            "Epoch: [10][14/38]\t Loss 0.1337(avg: 0.1570)\t Acc 0.952 (avg: 0.949)\n",
            "Epoch: [10][15/38]\t Loss 0.1547(avg: 0.1569)\t Acc 0.921 (avg: 0.947)\n",
            "Epoch: [10][16/38]\t Loss 0.2342(avg: 0.1615)\t Acc 0.952 (avg: 0.947)\n",
            "Epoch: [10][17/38]\t Loss 0.1777(avg: 0.1624)\t Acc 0.938 (avg: 0.947)\n",
            "Epoch: [10][18/38]\t Loss 0.1995(avg: 0.1642)\t Acc 0.933 (avg: 0.946)\n",
            "Epoch: [10][19/38]\t Loss 0.1344(avg: 0.1628)\t Acc 0.983 (avg: 0.948)\n",
            "Epoch: [10][20/38]\t Loss 0.1971(avg: 0.1644)\t Acc 0.935 (avg: 0.947)\n",
            "Epoch: [10][21/38]\t Loss 0.0796(avg: 0.1607)\t Acc 0.951 (avg: 0.947)\n",
            "Epoch: [10][22/38]\t Loss 0.2255(avg: 0.1634)\t Acc 0.933 (avg: 0.947)\n",
            "Epoch: [10][23/38]\t Loss 0.1047(avg: 0.1609)\t Acc 0.952 (avg: 0.947)\n",
            "Epoch: [10][24/38]\t Loss 0.1546(avg: 0.1607)\t Acc 0.967 (avg: 0.948)\n",
            "Epoch: [10][25/38]\t Loss 0.1634(avg: 0.1608)\t Acc 0.935 (avg: 0.947)\n",
            "Epoch: [10][26/38]\t Loss 0.1401(avg: 0.1600)\t Acc 0.952 (avg: 0.948)\n",
            "Epoch: [10][27/38]\t Loss 0.2158(avg: 0.1620)\t Acc 0.921 (avg: 0.947)\n",
            "Epoch: [10][28/38]\t Loss 0.2066(avg: 0.1636)\t Acc 0.889 (avg: 0.945)\n",
            "Epoch: [10][29/38]\t Loss 0.1309(avg: 0.1625)\t Acc 0.921 (avg: 0.944)\n",
            "Epoch: [10][30/38]\t Loss 0.2629(avg: 0.1658)\t Acc 0.891 (avg: 0.942)\n",
            "Epoch: [10][31/38]\t Loss 0.2336(avg: 0.1679)\t Acc 0.905 (avg: 0.941)\n",
            "Epoch: [10][32/38]\t Loss 0.1204(avg: 0.1665)\t Acc 0.952 (avg: 0.941)\n",
            "Epoch: [10][33/38]\t Loss 0.1886(avg: 0.1671)\t Acc 0.937 (avg: 0.941)\n",
            "Epoch: [10][34/38]\t Loss 0.1548(avg: 0.1668)\t Acc 0.952 (avg: 0.941)\n",
            "Epoch: [10][35/38]\t Loss 0.1599(avg: 0.1666)\t Acc 0.921 (avg: 0.941)\n",
            "Epoch: [10][36/38]\t Loss 0.2086(avg: 0.1677)\t Acc 0.921 (avg: 0.940)\n",
            "Epoch: [10][37/38]\t Loss 0.0065(avg: 0.1675)\t Acc 1.000 (avg: 0.940)\n",
            "Epoch: [10]\t Avg Loss 0.1675\t Avg Accuracy 0.940\n",
            "Test Average Accuracy: 0.7814\n",
            "Epoch: [11][0/38]\t Loss 0.0887(avg: 0.0887)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [11][1/38]\t Loss 0.0973(avg: 0.0930)\t Acc 0.968 (avg: 0.976)\n",
            "Epoch: [11][2/38]\t Loss 0.1521(avg: 0.1128)\t Acc 0.937 (avg: 0.963)\n",
            "Epoch: [11][3/38]\t Loss 0.0963(avg: 0.1088)\t Acc 0.950 (avg: 0.960)\n",
            "Epoch: [11][4/38]\t Loss 0.1762(avg: 0.1223)\t Acc 0.903 (avg: 0.948)\n",
            "Epoch: [11][5/38]\t Loss 0.0682(avg: 0.1133)\t Acc 0.984 (avg: 0.954)\n",
            "Epoch: [11][6/38]\t Loss 0.1597(avg: 0.1201)\t Acc 0.938 (avg: 0.952)\n",
            "Epoch: [11][7/38]\t Loss 0.0850(avg: 0.1157)\t Acc 0.984 (avg: 0.956)\n",
            "Epoch: [11][8/38]\t Loss 0.1591(avg: 0.1207)\t Acc 0.938 (avg: 0.954)\n",
            "Epoch: [11][9/38]\t Loss 0.0971(avg: 0.1184)\t Acc 0.984 (avg: 0.957)\n",
            "Epoch: [11][10/38]\t Loss 0.1354(avg: 0.1199)\t Acc 0.952 (avg: 0.956)\n",
            "Epoch: [11][11/38]\t Loss 0.0934(avg: 0.1177)\t Acc 0.952 (avg: 0.956)\n",
            "Epoch: [11][12/38]\t Loss 0.0867(avg: 0.1153)\t Acc 0.953 (avg: 0.956)\n",
            "Epoch: [11][13/38]\t Loss 0.1361(avg: 0.1167)\t Acc 0.919 (avg: 0.953)\n",
            "Epoch: [11][14/38]\t Loss 0.0913(avg: 0.1151)\t Acc 0.968 (avg: 0.954)\n",
            "Epoch: [11][15/38]\t Loss 0.1403(avg: 0.1167)\t Acc 0.922 (avg: 0.952)\n",
            "Epoch: [11][16/38]\t Loss 0.2258(avg: 0.1232)\t Acc 0.906 (avg: 0.949)\n",
            "Epoch: [11][17/38]\t Loss 0.2891(avg: 0.1327)\t Acc 0.906 (avg: 0.947)\n",
            "Epoch: [11][18/38]\t Loss 0.1008(avg: 0.1310)\t Acc 0.951 (avg: 0.947)\n",
            "Epoch: [11][19/38]\t Loss 0.1731(avg: 0.1331)\t Acc 0.921 (avg: 0.946)\n",
            "Epoch: [11][20/38]\t Loss 0.1440(avg: 0.1337)\t Acc 0.935 (avg: 0.945)\n",
            "Epoch: [11][21/38]\t Loss 0.1949(avg: 0.1364)\t Acc 0.935 (avg: 0.945)\n",
            "Epoch: [11][22/38]\t Loss 0.1288(avg: 0.1361)\t Acc 0.952 (avg: 0.945)\n",
            "Epoch: [11][23/38]\t Loss 0.2277(avg: 0.1400)\t Acc 0.953 (avg: 0.945)\n",
            "Epoch: [11][24/38]\t Loss 0.1499(avg: 0.1404)\t Acc 0.952 (avg: 0.946)\n",
            "Epoch: [11][25/38]\t Loss 0.0934(avg: 0.1386)\t Acc 0.984 (avg: 0.947)\n",
            "Epoch: [11][26/38]\t Loss 0.1607(avg: 0.1394)\t Acc 0.952 (avg: 0.947)\n",
            "Epoch: [11][27/38]\t Loss 0.1575(avg: 0.1400)\t Acc 0.937 (avg: 0.947)\n",
            "Epoch: [11][28/38]\t Loss 0.1177(avg: 0.1393)\t Acc 0.967 (avg: 0.948)\n",
            "Epoch: [11][29/38]\t Loss 0.0830(avg: 0.1374)\t Acc 0.984 (avg: 0.949)\n",
            "Epoch: [11][30/38]\t Loss 0.1447(avg: 0.1376)\t Acc 0.947 (avg: 0.949)\n",
            "Epoch: [11][31/38]\t Loss 0.1692(avg: 0.1386)\t Acc 0.922 (avg: 0.948)\n",
            "Epoch: [11][32/38]\t Loss 0.2289(avg: 0.1413)\t Acc 0.900 (avg: 0.947)\n",
            "Epoch: [11][33/38]\t Loss 0.1417(avg: 0.1413)\t Acc 0.935 (avg: 0.946)\n",
            "Epoch: [11][34/38]\t Loss 0.1874(avg: 0.1426)\t Acc 0.951 (avg: 0.946)\n",
            "Epoch: [11][35/38]\t Loss 0.0972(avg: 0.1413)\t Acc 0.984 (avg: 0.947)\n",
            "Epoch: [11][36/38]\t Loss 0.1876(avg: 0.1425)\t Acc 0.903 (avg: 0.946)\n",
            "Epoch: [11][37/38]\t Loss 0.1564(avg: 0.1425)\t Acc 1.000 (avg: 0.946)\n",
            "Epoch: [11]\t Avg Loss 0.1425\t Avg Accuracy 0.946\n",
            "Test Average Accuracy: 0.7722\n",
            "Epoch: [12][0/38]\t Loss 0.1339(avg: 0.1339)\t Acc 0.934 (avg: 0.934)\n",
            "Epoch: [12][1/38]\t Loss 0.2289(avg: 0.1818)\t Acc 0.903 (avg: 0.919)\n",
            "Epoch: [12][2/38]\t Loss 0.0879(avg: 0.1503)\t Acc 0.952 (avg: 0.930)\n",
            "Epoch: [12][3/38]\t Loss 0.0546(avg: 0.1260)\t Acc 0.984 (avg: 0.944)\n",
            "Epoch: [12][4/38]\t Loss 0.1191(avg: 0.1246)\t Acc 0.952 (avg: 0.945)\n",
            "Epoch: [12][5/38]\t Loss 0.1172(avg: 0.1233)\t Acc 0.953 (avg: 0.947)\n",
            "Epoch: [12][6/38]\t Loss 0.0903(avg: 0.1185)\t Acc 0.984 (avg: 0.952)\n",
            "Epoch: [12][7/38]\t Loss 0.1948(avg: 0.1281)\t Acc 0.921 (avg: 0.948)\n",
            "Epoch: [12][8/38]\t Loss 0.1446(avg: 0.1300)\t Acc 0.938 (avg: 0.947)\n",
            "Epoch: [12][9/38]\t Loss 0.1047(avg: 0.1275)\t Acc 0.952 (avg: 0.947)\n",
            "Epoch: [12][10/38]\t Loss 0.0957(avg: 0.1246)\t Acc 0.952 (avg: 0.948)\n",
            "Epoch: [12][11/38]\t Loss 0.1024(avg: 0.1228)\t Acc 0.951 (avg: 0.948)\n",
            "Epoch: [12][12/38]\t Loss 0.0583(avg: 0.1178)\t Acc 0.968 (avg: 0.950)\n",
            "Epoch: [12][13/38]\t Loss 0.2697(avg: 0.1287)\t Acc 0.905 (avg: 0.946)\n",
            "Epoch: [12][14/38]\t Loss 0.0403(avg: 0.1227)\t Acc 1.000 (avg: 0.950)\n",
            "Epoch: [12][15/38]\t Loss 0.1519(avg: 0.1245)\t Acc 0.938 (avg: 0.949)\n",
            "Epoch: [12][16/38]\t Loss 0.1788(avg: 0.1276)\t Acc 0.950 (avg: 0.949)\n",
            "Epoch: [12][17/38]\t Loss 0.1053(avg: 0.1264)\t Acc 0.918 (avg: 0.948)\n",
            "Epoch: [12][18/38]\t Loss 0.1060(avg: 0.1253)\t Acc 0.952 (avg: 0.948)\n",
            "Epoch: [12][19/38]\t Loss 0.2050(avg: 0.1291)\t Acc 0.932 (avg: 0.947)\n",
            "Epoch: [12][20/38]\t Loss 0.0871(avg: 0.1271)\t Acc 0.968 (avg: 0.948)\n",
            "Epoch: [12][21/38]\t Loss 0.1123(avg: 0.1264)\t Acc 0.933 (avg: 0.947)\n",
            "Epoch: [12][22/38]\t Loss 0.1348(avg: 0.1268)\t Acc 0.953 (avg: 0.948)\n",
            "Epoch: [12][23/38]\t Loss 0.2047(avg: 0.1301)\t Acc 0.938 (avg: 0.947)\n",
            "Epoch: [12][24/38]\t Loss 0.2335(avg: 0.1342)\t Acc 0.934 (avg: 0.947)\n",
            "Epoch: [12][25/38]\t Loss 0.0532(avg: 0.1311)\t Acc 1.000 (avg: 0.949)\n",
            "Epoch: [12][26/38]\t Loss 0.0515(avg: 0.1282)\t Acc 1.000 (avg: 0.951)\n",
            "Epoch: [12][27/38]\t Loss 0.1729(avg: 0.1297)\t Acc 0.952 (avg: 0.951)\n",
            "Epoch: [12][28/38]\t Loss 0.2800(avg: 0.1350)\t Acc 0.905 (avg: 0.949)\n",
            "Epoch: [12][29/38]\t Loss 0.2595(avg: 0.1392)\t Acc 0.922 (avg: 0.948)\n",
            "Epoch: [12][30/38]\t Loss 0.4612(avg: 0.1495)\t Acc 0.823 (avg: 0.944)\n",
            "Epoch: [12][31/38]\t Loss 0.1806(avg: 0.1505)\t Acc 0.921 (avg: 0.943)\n",
            "Epoch: [12][32/38]\t Loss 0.1246(avg: 0.1498)\t Acc 0.950 (avg: 0.944)\n",
            "Epoch: [12][33/38]\t Loss 0.1101(avg: 0.1486)\t Acc 0.935 (avg: 0.943)\n",
            "Epoch: [12][34/38]\t Loss 0.1622(avg: 0.1490)\t Acc 0.952 (avg: 0.944)\n",
            "Epoch: [12][35/38]\t Loss 0.1755(avg: 0.1497)\t Acc 0.903 (avg: 0.943)\n",
            "Epoch: [12][36/38]\t Loss 0.3025(avg: 0.1539)\t Acc 0.921 (avg: 0.942)\n",
            "Epoch: [12][37/38]\t Loss 0.3286(avg: 0.1543)\t Acc 0.800 (avg: 0.942)\n",
            "Epoch: [12]\t Avg Loss 0.1543\t Avg Accuracy 0.942\n",
            "Test Average Accuracy: 0.7689\n",
            "Epoch: [13][0/38]\t Loss 0.0661(avg: 0.0661)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [13][1/38]\t Loss 0.0991(avg: 0.0829)\t Acc 0.952 (avg: 0.968)\n",
            "Epoch: [13][2/38]\t Loss 0.0587(avg: 0.0750)\t Acc 0.983 (avg: 0.973)\n",
            "Epoch: [13][3/38]\t Loss 0.1675(avg: 0.0977)\t Acc 0.917 (avg: 0.959)\n",
            "Epoch: [13][4/38]\t Loss 0.1371(avg: 0.1058)\t Acc 0.937 (avg: 0.954)\n",
            "Epoch: [13][5/38]\t Loss 0.0656(avg: 0.0990)\t Acc 0.952 (avg: 0.954)\n",
            "Epoch: [13][6/38]\t Loss 0.0553(avg: 0.0928)\t Acc 1.000 (avg: 0.961)\n",
            "Epoch: [13][7/38]\t Loss 0.0661(avg: 0.0894)\t Acc 0.968 (avg: 0.961)\n",
            "Epoch: [13][8/38]\t Loss 0.0598(avg: 0.0861)\t Acc 0.984 (avg: 0.964)\n",
            "Epoch: [13][9/38]\t Loss 0.0505(avg: 0.0824)\t Acc 0.969 (avg: 0.964)\n",
            "Epoch: [13][10/38]\t Loss 0.2553(avg: 0.0986)\t Acc 0.938 (avg: 0.962)\n",
            "Epoch: [13][11/38]\t Loss 0.1127(avg: 0.0998)\t Acc 0.968 (avg: 0.962)\n",
            "Epoch: [13][12/38]\t Loss 0.1265(avg: 0.1019)\t Acc 0.953 (avg: 0.962)\n",
            "Epoch: [13][13/38]\t Loss 0.1754(avg: 0.1072)\t Acc 0.937 (avg: 0.960)\n",
            "Epoch: [13][14/38]\t Loss 0.0542(avg: 0.1036)\t Acc 0.984 (avg: 0.961)\n",
            "Epoch: [13][15/38]\t Loss 0.1362(avg: 0.1057)\t Acc 0.952 (avg: 0.961)\n",
            "Epoch: [13][16/38]\t Loss 0.0640(avg: 0.1033)\t Acc 0.984 (avg: 0.962)\n",
            "Epoch: [13][17/38]\t Loss 0.2468(avg: 0.1112)\t Acc 0.935 (avg: 0.961)\n",
            "Epoch: [13][18/38]\t Loss 0.0596(avg: 0.1085)\t Acc 0.984 (avg: 0.962)\n",
            "Epoch: [13][19/38]\t Loss 0.1489(avg: 0.1105)\t Acc 0.968 (avg: 0.962)\n",
            "Epoch: [13][20/38]\t Loss 0.1393(avg: 0.1119)\t Acc 0.921 (avg: 0.960)\n",
            "Epoch: [13][21/38]\t Loss 0.3118(avg: 0.1213)\t Acc 0.922 (avg: 0.958)\n",
            "Epoch: [13][22/38]\t Loss 0.5565(avg: 0.1404)\t Acc 0.810 (avg: 0.952)\n",
            "Epoch: [13][23/38]\t Loss 0.1423(avg: 0.1404)\t Acc 0.950 (avg: 0.952)\n",
            "Epoch: [13][24/38]\t Loss 0.0551(avg: 0.1369)\t Acc 0.969 (avg: 0.953)\n",
            "Epoch: [13][25/38]\t Loss 0.0808(avg: 0.1348)\t Acc 0.984 (avg: 0.954)\n",
            "Epoch: [13][26/38]\t Loss 0.1749(avg: 0.1363)\t Acc 0.937 (avg: 0.953)\n",
            "Epoch: [13][27/38]\t Loss 0.1710(avg: 0.1375)\t Acc 0.921 (avg: 0.952)\n",
            "Epoch: [13][28/38]\t Loss 0.0929(avg: 0.1360)\t Acc 0.967 (avg: 0.952)\n",
            "Epoch: [13][29/38]\t Loss 0.0481(avg: 0.1331)\t Acc 0.968 (avg: 0.953)\n",
            "Epoch: [13][30/38]\t Loss 0.1366(avg: 0.1332)\t Acc 0.951 (avg: 0.953)\n",
            "Epoch: [13][31/38]\t Loss 0.1350(avg: 0.1333)\t Acc 0.952 (avg: 0.953)\n",
            "Epoch: [13][32/38]\t Loss 0.1626(avg: 0.1342)\t Acc 0.952 (avg: 0.953)\n",
            "Epoch: [13][33/38]\t Loss 0.1817(avg: 0.1356)\t Acc 0.919 (avg: 0.952)\n",
            "Epoch: [13][34/38]\t Loss 0.1330(avg: 0.1355)\t Acc 0.968 (avg: 0.952)\n",
            "Epoch: [13][35/38]\t Loss 0.1854(avg: 0.1369)\t Acc 0.952 (avg: 0.952)\n",
            "Epoch: [13][36/38]\t Loss 0.0852(avg: 0.1355)\t Acc 0.952 (avg: 0.952)\n",
            "Epoch: [13][37/38]\t Loss 0.0682(avg: 0.1353)\t Acc 1.000 (avg: 0.952)\n",
            "Epoch: [13]\t Avg Loss 0.1353\t Avg Accuracy 0.952\n",
            "Test Average Accuracy: 0.7708\n",
            "Epoch: [14][0/38]\t Loss 0.1807(avg: 0.1807)\t Acc 0.935 (avg: 0.935)\n",
            "Epoch: [14][1/38]\t Loss 0.0660(avg: 0.1229)\t Acc 0.968 (avg: 0.952)\n",
            "Epoch: [14][2/38]\t Loss 0.0978(avg: 0.1145)\t Acc 0.984 (avg: 0.963)\n",
            "Epoch: [14][3/38]\t Loss 0.0603(avg: 0.1012)\t Acc 0.967 (avg: 0.964)\n",
            "Epoch: [14][4/38]\t Loss 0.1651(avg: 0.1141)\t Acc 0.968 (avg: 0.965)\n",
            "Epoch: [14][5/38]\t Loss 0.0439(avg: 0.1023)\t Acc 1.000 (avg: 0.971)\n",
            "Epoch: [14][6/38]\t Loss 0.2119(avg: 0.1179)\t Acc 0.919 (avg: 0.963)\n",
            "Epoch: [14][7/38]\t Loss 0.1565(avg: 0.1227)\t Acc 0.937 (avg: 0.960)\n",
            "Epoch: [14][8/38]\t Loss 0.1336(avg: 0.1239)\t Acc 0.951 (avg: 0.959)\n",
            "Epoch: [14][9/38]\t Loss 0.0907(avg: 0.1205)\t Acc 0.969 (avg: 0.960)\n",
            "Epoch: [14][10/38]\t Loss 0.0583(avg: 0.1149)\t Acc 0.984 (avg: 0.962)\n",
            "Epoch: [14][11/38]\t Loss 0.0914(avg: 0.1130)\t Acc 0.952 (avg: 0.961)\n",
            "Epoch: [14][12/38]\t Loss 0.0781(avg: 0.1104)\t Acc 0.966 (avg: 0.962)\n",
            "Epoch: [14][13/38]\t Loss 0.1074(avg: 0.1102)\t Acc 0.984 (avg: 0.963)\n",
            "Epoch: [14][14/38]\t Loss 0.1846(avg: 0.1152)\t Acc 0.935 (avg: 0.961)\n",
            "Epoch: [14][15/38]\t Loss 0.1090(avg: 0.1148)\t Acc 0.952 (avg: 0.961)\n",
            "Epoch: [14][16/38]\t Loss 0.1594(avg: 0.1174)\t Acc 0.937 (avg: 0.959)\n",
            "Epoch: [14][17/38]\t Loss 0.2348(avg: 0.1239)\t Acc 0.918 (avg: 0.957)\n",
            "Epoch: [14][18/38]\t Loss 0.1943(avg: 0.1277)\t Acc 0.953 (avg: 0.957)\n",
            "Epoch: [14][19/38]\t Loss 0.1382(avg: 0.1282)\t Acc 0.952 (avg: 0.957)\n",
            "Epoch: [14][20/38]\t Loss 0.0735(avg: 0.1255)\t Acc 0.984 (avg: 0.958)\n",
            "Epoch: [14][21/38]\t Loss 0.0342(avg: 0.1213)\t Acc 1.000 (avg: 0.960)\n",
            "Epoch: [14][22/38]\t Loss 0.1256(avg: 0.1215)\t Acc 0.968 (avg: 0.960)\n",
            "Epoch: [14][23/38]\t Loss 0.1581(avg: 0.1230)\t Acc 0.968 (avg: 0.961)\n",
            "Epoch: [14][24/38]\t Loss 0.1726(avg: 0.1249)\t Acc 0.902 (avg: 0.958)\n",
            "Epoch: [14][25/38]\t Loss 0.0492(avg: 0.1220)\t Acc 0.984 (avg: 0.959)\n",
            "Epoch: [14][26/38]\t Loss 0.1127(avg: 0.1217)\t Acc 0.933 (avg: 0.958)\n",
            "Epoch: [14][27/38]\t Loss 0.1054(avg: 0.1211)\t Acc 0.938 (avg: 0.958)\n",
            "Epoch: [14][28/38]\t Loss 0.0901(avg: 0.1200)\t Acc 0.984 (avg: 0.959)\n",
            "Epoch: [14][29/38]\t Loss 0.0951(avg: 0.1192)\t Acc 0.951 (avg: 0.958)\n",
            "Epoch: [14][30/38]\t Loss 0.1104(avg: 0.1189)\t Acc 0.952 (avg: 0.958)\n",
            "Epoch: [14][31/38]\t Loss 0.0688(avg: 0.1173)\t Acc 0.984 (avg: 0.959)\n",
            "Epoch: [14][32/38]\t Loss 0.0982(avg: 0.1167)\t Acc 0.984 (avg: 0.960)\n",
            "Epoch: [14][33/38]\t Loss 0.1445(avg: 0.1176)\t Acc 0.952 (avg: 0.959)\n",
            "Epoch: [14][34/38]\t Loss 0.1115(avg: 0.1174)\t Acc 0.952 (avg: 0.959)\n",
            "Epoch: [14][35/38]\t Loss 0.0821(avg: 0.1165)\t Acc 0.983 (avg: 0.960)\n",
            "Epoch: [14][36/38]\t Loss 0.0415(avg: 0.1144)\t Acc 0.984 (avg: 0.961)\n",
            "Epoch: [14][37/38]\t Loss 0.0056(avg: 0.1142)\t Acc 1.000 (avg: 0.961)\n",
            "Epoch: [14]\t Avg Loss 0.1142\t Avg Accuracy 0.961\n",
            "Test Average Accuracy: 0.7768\n",
            "Epoch: [15][0/38]\t Loss 0.0524(avg: 0.0524)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [15][1/38]\t Loss 0.0709(avg: 0.0615)\t Acc 0.983 (avg: 0.984)\n",
            "Epoch: [15][2/38]\t Loss 0.0437(avg: 0.0555)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [15][3/38]\t Loss 0.0824(avg: 0.0624)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [15][4/38]\t Loss 0.1102(avg: 0.0718)\t Acc 0.951 (avg: 0.974)\n",
            "Epoch: [15][5/38]\t Loss 0.0728(avg: 0.0720)\t Acc 0.968 (avg: 0.973)\n",
            "Epoch: [15][6/38]\t Loss 0.0819(avg: 0.0734)\t Acc 0.983 (avg: 0.974)\n",
            "Epoch: [15][7/38]\t Loss 0.0346(avg: 0.0685)\t Acc 1.000 (avg: 0.978)\n",
            "Epoch: [15][8/38]\t Loss 0.0354(avg: 0.0648)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [15][9/38]\t Loss 0.0114(avg: 0.0595)\t Acc 1.000 (avg: 0.982)\n",
            "Epoch: [15][10/38]\t Loss 0.1168(avg: 0.0647)\t Acc 0.968 (avg: 0.981)\n",
            "Epoch: [15][11/38]\t Loss 0.1348(avg: 0.0706)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [15][12/38]\t Loss 0.0660(avg: 0.0702)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [15][13/38]\t Loss 0.0345(avg: 0.0676)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [15][14/38]\t Loss 0.0969(avg: 0.0696)\t Acc 0.969 (avg: 0.980)\n",
            "Epoch: [15][15/38]\t Loss 0.1471(avg: 0.0745)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [15][16/38]\t Loss 0.0492(avg: 0.0731)\t Acc 0.968 (avg: 0.978)\n",
            "Epoch: [15][17/38]\t Loss 0.1572(avg: 0.0778)\t Acc 0.952 (avg: 0.977)\n",
            "Epoch: [15][18/38]\t Loss 0.1933(avg: 0.0839)\t Acc 0.887 (avg: 0.972)\n",
            "Epoch: [15][19/38]\t Loss 0.1308(avg: 0.0862)\t Acc 0.903 (avg: 0.969)\n",
            "Epoch: [15][20/38]\t Loss 0.1715(avg: 0.0903)\t Acc 0.952 (avg: 0.968)\n",
            "Epoch: [15][21/38]\t Loss 0.2473(avg: 0.0975)\t Acc 0.905 (avg: 0.965)\n",
            "Epoch: [15][22/38]\t Loss 0.1073(avg: 0.0980)\t Acc 0.968 (avg: 0.965)\n",
            "Epoch: [15][23/38]\t Loss 0.0906(avg: 0.0977)\t Acc 0.968 (avg: 0.965)\n",
            "Epoch: [15][24/38]\t Loss 0.1254(avg: 0.0988)\t Acc 0.953 (avg: 0.965)\n",
            "Epoch: [15][25/38]\t Loss 0.1025(avg: 0.0989)\t Acc 0.952 (avg: 0.964)\n",
            "Epoch: [15][26/38]\t Loss 0.1628(avg: 0.1014)\t Acc 0.953 (avg: 0.964)\n",
            "Epoch: [15][27/38]\t Loss 0.1783(avg: 0.1041)\t Acc 0.937 (avg: 0.963)\n",
            "Epoch: [15][28/38]\t Loss 0.2102(avg: 0.1079)\t Acc 0.922 (avg: 0.961)\n",
            "Epoch: [15][29/38]\t Loss 0.1072(avg: 0.1079)\t Acc 0.951 (avg: 0.961)\n",
            "Epoch: [15][30/38]\t Loss 0.0532(avg: 0.1061)\t Acc 0.984 (avg: 0.962)\n",
            "Epoch: [15][31/38]\t Loss 0.1028(avg: 0.1060)\t Acc 0.967 (avg: 0.962)\n",
            "Epoch: [15][32/38]\t Loss 0.0330(avg: 0.1037)\t Acc 0.984 (avg: 0.963)\n",
            "Epoch: [15][33/38]\t Loss 0.1238(avg: 0.1043)\t Acc 0.952 (avg: 0.962)\n",
            "Epoch: [15][34/38]\t Loss 0.0963(avg: 0.1041)\t Acc 0.952 (avg: 0.962)\n",
            "Epoch: [15][35/38]\t Loss 0.1116(avg: 0.1043)\t Acc 0.968 (avg: 0.962)\n",
            "Epoch: [15][36/38]\t Loss 0.0658(avg: 0.1033)\t Acc 0.967 (avg: 0.962)\n",
            "Epoch: [15][37/38]\t Loss 0.0014(avg: 0.1031)\t Acc 1.000 (avg: 0.962)\n",
            "Epoch: [15]\t Avg Loss 0.1031\t Avg Accuracy 0.962\n",
            "Test Average Accuracy: 0.7820\n",
            "Epoch: [16][0/38]\t Loss 0.0437(avg: 0.0437)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [16][1/38]\t Loss 0.0503(avg: 0.0470)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [16][2/38]\t Loss 0.0675(avg: 0.0538)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [16][3/38]\t Loss 0.1334(avg: 0.0739)\t Acc 0.952 (avg: 0.972)\n",
            "Epoch: [16][4/38]\t Loss 0.0474(avg: 0.0685)\t Acc 0.984 (avg: 0.975)\n",
            "Epoch: [16][5/38]\t Loss 0.1032(avg: 0.0742)\t Acc 0.935 (avg: 0.968)\n",
            "Epoch: [16][6/38]\t Loss 0.1503(avg: 0.0851)\t Acc 0.937 (avg: 0.964)\n",
            "Epoch: [16][7/38]\t Loss 0.2179(avg: 0.1016)\t Acc 0.952 (avg: 0.962)\n",
            "Epoch: [16][8/38]\t Loss 0.0403(avg: 0.0948)\t Acc 1.000 (avg: 0.966)\n",
            "Epoch: [16][9/38]\t Loss 0.1748(avg: 0.1028)\t Acc 0.935 (avg: 0.963)\n",
            "Epoch: [16][10/38]\t Loss 0.0837(avg: 0.1010)\t Acc 0.952 (avg: 0.962)\n",
            "Epoch: [16][11/38]\t Loss 0.0491(avg: 0.0967)\t Acc 0.984 (avg: 0.964)\n",
            "Epoch: [16][12/38]\t Loss 0.0359(avg: 0.0919)\t Acc 0.984 (avg: 0.966)\n",
            "Epoch: [16][13/38]\t Loss 0.0369(avg: 0.0879)\t Acc 0.984 (avg: 0.967)\n",
            "Epoch: [16][14/38]\t Loss 0.0558(avg: 0.0857)\t Acc 0.969 (avg: 0.967)\n",
            "Epoch: [16][15/38]\t Loss 0.1196(avg: 0.0879)\t Acc 0.968 (avg: 0.967)\n",
            "Epoch: [16][16/38]\t Loss 0.1183(avg: 0.0897)\t Acc 0.952 (avg: 0.966)\n",
            "Epoch: [16][17/38]\t Loss 0.1292(avg: 0.0917)\t Acc 0.949 (avg: 0.965)\n",
            "Epoch: [16][18/38]\t Loss 0.0651(avg: 0.0904)\t Acc 0.968 (avg: 0.965)\n",
            "Epoch: [16][19/38]\t Loss 0.0576(avg: 0.0887)\t Acc 0.984 (avg: 0.966)\n",
            "Epoch: [16][20/38]\t Loss 0.0758(avg: 0.0881)\t Acc 0.983 (avg: 0.967)\n",
            "Epoch: [16][21/38]\t Loss 0.1379(avg: 0.0904)\t Acc 0.952 (avg: 0.966)\n",
            "Epoch: [16][22/38]\t Loss 0.1512(avg: 0.0931)\t Acc 0.938 (avg: 0.965)\n",
            "Epoch: [16][23/38]\t Loss 0.0643(avg: 0.0919)\t Acc 0.968 (avg: 0.965)\n",
            "Epoch: [16][24/38]\t Loss 0.0167(avg: 0.0888)\t Acc 1.000 (avg: 0.967)\n",
            "Epoch: [16][25/38]\t Loss 0.0333(avg: 0.0868)\t Acc 0.983 (avg: 0.967)\n",
            "Epoch: [16][26/38]\t Loss 0.0484(avg: 0.0854)\t Acc 0.984 (avg: 0.968)\n",
            "Epoch: [16][27/38]\t Loss 0.0329(avg: 0.0835)\t Acc 1.000 (avg: 0.969)\n",
            "Epoch: [16][28/38]\t Loss 0.0438(avg: 0.0821)\t Acc 1.000 (avg: 0.970)\n",
            "Epoch: [16][29/38]\t Loss 0.0453(avg: 0.0809)\t Acc 0.983 (avg: 0.971)\n",
            "Epoch: [16][30/38]\t Loss 0.0843(avg: 0.0810)\t Acc 0.969 (avg: 0.971)\n",
            "Epoch: [16][31/38]\t Loss 0.1583(avg: 0.0835)\t Acc 0.952 (avg: 0.970)\n",
            "Epoch: [16][32/38]\t Loss 0.0835(avg: 0.0835)\t Acc 0.968 (avg: 0.970)\n",
            "Epoch: [16][33/38]\t Loss 0.0688(avg: 0.0831)\t Acc 0.967 (avg: 0.970)\n",
            "Epoch: [16][34/38]\t Loss 0.0747(avg: 0.0828)\t Acc 0.969 (avg: 0.970)\n",
            "Epoch: [16][35/38]\t Loss 0.0477(avg: 0.0819)\t Acc 0.967 (avg: 0.970)\n",
            "Epoch: [16][36/38]\t Loss 0.0790(avg: 0.0818)\t Acc 0.984 (avg: 0.970)\n",
            "Epoch: [16][37/38]\t Loss 0.0123(avg: 0.0817)\t Acc 1.000 (avg: 0.970)\n",
            "Epoch: [16]\t Avg Loss 0.0817\t Avg Accuracy 0.970\n",
            "Test Average Accuracy: 0.7879\n",
            "Epoch: [17][0/38]\t Loss 0.0475(avg: 0.0475)\t Acc 0.968 (avg: 0.968)\n",
            "Epoch: [17][1/38]\t Loss 0.0478(avg: 0.0476)\t Acc 0.984 (avg: 0.976)\n",
            "Epoch: [17][2/38]\t Loss 0.0676(avg: 0.0542)\t Acc 0.968 (avg: 0.973)\n",
            "Epoch: [17][3/38]\t Loss 0.0444(avg: 0.0518)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [17][4/38]\t Loss 0.0343(avg: 0.0482)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [17][5/38]\t Loss 0.1326(avg: 0.0624)\t Acc 0.937 (avg: 0.976)\n",
            "Epoch: [17][6/38]\t Loss 0.1029(avg: 0.0683)\t Acc 0.953 (avg: 0.973)\n",
            "Epoch: [17][7/38]\t Loss 0.0399(avg: 0.0647)\t Acc 0.984 (avg: 0.974)\n",
            "Epoch: [17][8/38]\t Loss 0.0599(avg: 0.0641)\t Acc 0.984 (avg: 0.975)\n",
            "Epoch: [17][9/38]\t Loss 0.0282(avg: 0.0608)\t Acc 0.983 (avg: 0.976)\n",
            "Epoch: [17][10/38]\t Loss 0.0354(avg: 0.0585)\t Acc 0.984 (avg: 0.977)\n",
            "Epoch: [17][11/38]\t Loss 0.0510(avg: 0.0578)\t Acc 0.984 (avg: 0.977)\n",
            "Epoch: [17][12/38]\t Loss 0.1530(avg: 0.0652)\t Acc 0.952 (avg: 0.975)\n",
            "Epoch: [17][13/38]\t Loss 0.0607(avg: 0.0649)\t Acc 0.967 (avg: 0.975)\n",
            "Epoch: [17][14/38]\t Loss 0.0281(avg: 0.0625)\t Acc 1.000 (avg: 0.976)\n",
            "Epoch: [17][15/38]\t Loss 0.0330(avg: 0.0606)\t Acc 1.000 (avg: 0.978)\n",
            "Epoch: [17][16/38]\t Loss 0.0274(avg: 0.0588)\t Acc 0.983 (avg: 0.978)\n",
            "Epoch: [17][17/38]\t Loss 0.0403(avg: 0.0577)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [17][18/38]\t Loss 0.0460(avg: 0.0571)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [17][19/38]\t Loss 0.1609(avg: 0.0624)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [17][20/38]\t Loss 0.0595(avg: 0.0622)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [17][21/38]\t Loss 0.0637(avg: 0.0623)\t Acc 0.967 (avg: 0.979)\n",
            "Epoch: [17][22/38]\t Loss 0.0273(avg: 0.0607)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [17][23/38]\t Loss 0.0227(avg: 0.0591)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [17][24/38]\t Loss 0.0488(avg: 0.0587)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [17][25/38]\t Loss 0.0197(avg: 0.0572)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [17][26/38]\t Loss 0.0693(avg: 0.0577)\t Acc 0.984 (avg: 0.980)\n",
            "Epoch: [17][27/38]\t Loss 0.1271(avg: 0.0601)\t Acc 0.967 (avg: 0.980)\n",
            "Epoch: [17][28/38]\t Loss 0.1362(avg: 0.0627)\t Acc 0.952 (avg: 0.979)\n",
            "Epoch: [17][29/38]\t Loss 0.1300(avg: 0.0649)\t Acc 0.967 (avg: 0.979)\n",
            "Epoch: [17][30/38]\t Loss 0.1010(avg: 0.0660)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [17][31/38]\t Loss 0.1121(avg: 0.0675)\t Acc 0.952 (avg: 0.978)\n",
            "Epoch: [17][32/38]\t Loss 0.0693(avg: 0.0675)\t Acc 1.000 (avg: 0.979)\n",
            "Epoch: [17][33/38]\t Loss 0.1612(avg: 0.0704)\t Acc 0.938 (avg: 0.977)\n",
            "Epoch: [17][34/38]\t Loss 0.0225(avg: 0.0690)\t Acc 1.000 (avg: 0.978)\n",
            "Epoch: [17][35/38]\t Loss 0.0641(avg: 0.0688)\t Acc 0.984 (avg: 0.978)\n",
            "Epoch: [17][36/38]\t Loss 0.2386(avg: 0.0735)\t Acc 0.952 (avg: 0.977)\n",
            "Epoch: [17][37/38]\t Loss 0.6246(avg: 0.0747)\t Acc 0.800 (avg: 0.977)\n",
            "Epoch: [17]\t Avg Loss 0.0747\t Avg Accuracy 0.977\n",
            "Test Average Accuracy: 0.7768\n",
            "Epoch: [18][0/38]\t Loss 0.0118(avg: 0.0118)\t Acc 1.000 (avg: 1.000)\n",
            "Epoch: [18][1/38]\t Loss 0.1051(avg: 0.0569)\t Acc 0.949 (avg: 0.975)\n",
            "Epoch: [18][2/38]\t Loss 0.0077(avg: 0.0400)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [18][3/38]\t Loss 0.0655(avg: 0.0464)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [18][4/38]\t Loss 0.0164(avg: 0.0404)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [18][5/38]\t Loss 0.0135(avg: 0.0358)\t Acc 1.000 (avg: 0.987)\n",
            "Epoch: [18][6/38]\t Loss 0.0501(avg: 0.0379)\t Acc 0.969 (avg: 0.984)\n",
            "Epoch: [18][7/38]\t Loss 0.0716(avg: 0.0421)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [18][8/38]\t Loss 0.0241(avg: 0.0401)\t Acc 1.000 (avg: 0.986)\n",
            "Epoch: [18][9/38]\t Loss 0.1218(avg: 0.0481)\t Acc 0.952 (avg: 0.983)\n",
            "Epoch: [18][10/38]\t Loss 0.0656(avg: 0.0497)\t Acc 0.952 (avg: 0.980)\n",
            "Epoch: [18][11/38]\t Loss 0.1252(avg: 0.0561)\t Acc 0.969 (avg: 0.979)\n",
            "Epoch: [18][12/38]\t Loss 0.0350(avg: 0.0544)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [18][13/38]\t Loss 0.0504(avg: 0.0542)\t Acc 0.969 (avg: 0.979)\n",
            "Epoch: [18][14/38]\t Loss 0.0188(avg: 0.0519)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [18][15/38]\t Loss 0.1055(avg: 0.0552)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [18][16/38]\t Loss 0.0534(avg: 0.0551)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [18][17/38]\t Loss 0.0617(avg: 0.0554)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [18][18/38]\t Loss 0.1740(avg: 0.0615)\t Acc 0.951 (avg: 0.977)\n",
            "Epoch: [18][19/38]\t Loss 0.0599(avg: 0.0614)\t Acc 0.984 (avg: 0.978)\n",
            "Epoch: [18][20/38]\t Loss 0.0275(avg: 0.0598)\t Acc 1.000 (avg: 0.979)\n",
            "Epoch: [18][21/38]\t Loss 0.0204(avg: 0.0580)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [18][22/38]\t Loss 0.0075(avg: 0.0558)\t Acc 1.000 (avg: 0.981)\n",
            "Epoch: [18][23/38]\t Loss 0.0269(avg: 0.0546)\t Acc 0.983 (avg: 0.981)\n",
            "Epoch: [18][24/38]\t Loss 0.0744(avg: 0.0554)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [18][25/38]\t Loss 0.0674(avg: 0.0559)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [18][26/38]\t Loss 0.0857(avg: 0.0570)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [18][27/38]\t Loss 0.0454(avg: 0.0566)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [18][28/38]\t Loss 0.1303(avg: 0.0590)\t Acc 0.967 (avg: 0.979)\n",
            "Epoch: [18][29/38]\t Loss 0.0375(avg: 0.0583)\t Acc 0.984 (avg: 0.979)\n",
            "Epoch: [18][30/38]\t Loss 0.0079(avg: 0.0568)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [18][31/38]\t Loss 0.0575(avg: 0.0568)\t Acc 0.984 (avg: 0.980)\n",
            "Epoch: [18][32/38]\t Loss 0.1148(avg: 0.0585)\t Acc 0.934 (avg: 0.979)\n",
            "Epoch: [18][33/38]\t Loss 0.1870(avg: 0.0624)\t Acc 0.921 (avg: 0.977)\n",
            "Epoch: [18][34/38]\t Loss 0.2887(avg: 0.0687)\t Acc 0.918 (avg: 0.975)\n",
            "Epoch: [18][35/38]\t Loss 0.1449(avg: 0.0708)\t Acc 0.951 (avg: 0.975)\n",
            "Epoch: [18][36/38]\t Loss 0.0118(avg: 0.0691)\t Acc 1.000 (avg: 0.975)\n",
            "Epoch: [18][37/38]\t Loss 0.0035(avg: 0.0690)\t Acc 1.000 (avg: 0.975)\n",
            "Epoch: [18]\t Avg Loss 0.0690\t Avg Accuracy 0.975\n",
            "Test Average Accuracy: 0.7741\n",
            "Epoch: [19][0/38]\t Loss 0.0494(avg: 0.0494)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [19][1/38]\t Loss 0.0502(avg: 0.0498)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [19][2/38]\t Loss 0.0527(avg: 0.0507)\t Acc 0.968 (avg: 0.979)\n",
            "Epoch: [19][3/38]\t Loss 0.2881(avg: 0.1094)\t Acc 0.952 (avg: 0.972)\n",
            "Epoch: [19][4/38]\t Loss 0.1300(avg: 0.1135)\t Acc 0.984 (avg: 0.974)\n",
            "Epoch: [19][5/38]\t Loss 0.0687(avg: 0.1062)\t Acc 0.984 (avg: 0.976)\n",
            "Epoch: [19][6/38]\t Loss 0.0313(avg: 0.0952)\t Acc 0.984 (avg: 0.977)\n",
            "Epoch: [19][7/38]\t Loss 0.0881(avg: 0.0943)\t Acc 0.967 (avg: 0.976)\n",
            "Epoch: [19][8/38]\t Loss 0.0322(avg: 0.0877)\t Acc 1.000 (avg: 0.979)\n",
            "Epoch: [19][9/38]\t Loss 0.0769(avg: 0.0866)\t Acc 0.968 (avg: 0.977)\n",
            "Epoch: [19][10/38]\t Loss 0.0409(avg: 0.0826)\t Acc 0.983 (avg: 0.978)\n",
            "Epoch: [19][11/38]\t Loss 0.0731(avg: 0.0818)\t Acc 0.984 (avg: 0.978)\n",
            "Epoch: [19][12/38]\t Loss 0.0051(avg: 0.0758)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [19][13/38]\t Loss 0.0290(avg: 0.0725)\t Acc 0.984 (avg: 0.980)\n",
            "Epoch: [19][14/38]\t Loss 0.0149(avg: 0.0686)\t Acc 1.000 (avg: 0.982)\n",
            "Epoch: [19][15/38]\t Loss 0.0293(avg: 0.0662)\t Acc 0.984 (avg: 0.982)\n",
            "Epoch: [19][16/38]\t Loss 0.0157(avg: 0.0631)\t Acc 1.000 (avg: 0.983)\n",
            "Epoch: [19][17/38]\t Loss 0.0892(avg: 0.0646)\t Acc 0.968 (avg: 0.982)\n",
            "Epoch: [19][18/38]\t Loss 0.0103(avg: 0.0617)\t Acc 1.000 (avg: 0.983)\n",
            "Epoch: [19][19/38]\t Loss 0.0256(avg: 0.0599)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [19][20/38]\t Loss 0.0442(avg: 0.0591)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [19][21/38]\t Loss 0.0379(avg: 0.0582)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [19][22/38]\t Loss 0.1241(avg: 0.0610)\t Acc 0.935 (avg: 0.982)\n",
            "Epoch: [19][23/38]\t Loss 0.0283(avg: 0.0597)\t Acc 0.984 (avg: 0.982)\n",
            "Epoch: [19][24/38]\t Loss 0.1251(avg: 0.0623)\t Acc 0.968 (avg: 0.981)\n",
            "Epoch: [19][25/38]\t Loss 0.1898(avg: 0.0673)\t Acc 0.937 (avg: 0.980)\n",
            "Epoch: [19][26/38]\t Loss 0.0962(avg: 0.0683)\t Acc 0.984 (avg: 0.980)\n",
            "Epoch: [19][27/38]\t Loss 0.0256(avg: 0.0668)\t Acc 0.984 (avg: 0.980)\n",
            "Epoch: [19][28/38]\t Loss 0.0309(avg: 0.0656)\t Acc 0.984 (avg: 0.980)\n",
            "Epoch: [19][29/38]\t Loss 0.0170(avg: 0.0640)\t Acc 1.000 (avg: 0.981)\n",
            "Epoch: [19][30/38]\t Loss 0.0865(avg: 0.0647)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [19][31/38]\t Loss 0.2424(avg: 0.0703)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [19][32/38]\t Loss 0.0224(avg: 0.0688)\t Acc 1.000 (avg: 0.981)\n",
            "Epoch: [19][33/38]\t Loss 0.0415(avg: 0.0680)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [19][34/38]\t Loss 0.1909(avg: 0.0714)\t Acc 0.933 (avg: 0.979)\n",
            "Epoch: [19][35/38]\t Loss 0.2490(avg: 0.0765)\t Acc 0.922 (avg: 0.977)\n",
            "Epoch: [19][36/38]\t Loss 0.0556(avg: 0.0759)\t Acc 0.984 (avg: 0.977)\n",
            "Epoch: [19][37/38]\t Loss 0.0320(avg: 0.0758)\t Acc 1.000 (avg: 0.978)\n",
            "Epoch: [19]\t Avg Loss 0.0758\t Avg Accuracy 0.978\n",
            "Test Average Accuracy: 0.7735\n",
            "Epoch: [20][0/38]\t Loss 0.0360(avg: 0.0360)\t Acc 1.000 (avg: 1.000)\n",
            "Epoch: [20][1/38]\t Loss 0.1402(avg: 0.0881)\t Acc 0.968 (avg: 0.984)\n",
            "Epoch: [20][2/38]\t Loss 0.0266(avg: 0.0674)\t Acc 1.000 (avg: 0.989)\n",
            "Epoch: [20][3/38]\t Loss 0.0437(avg: 0.0618)\t Acc 0.983 (avg: 0.988)\n",
            "Epoch: [20][4/38]\t Loss 0.0992(avg: 0.0693)\t Acc 0.968 (avg: 0.984)\n",
            "Epoch: [20][5/38]\t Loss 0.0699(avg: 0.0694)\t Acc 0.968 (avg: 0.981)\n",
            "Epoch: [20][6/38]\t Loss 0.0566(avg: 0.0676)\t Acc 0.952 (avg: 0.977)\n",
            "Epoch: [20][7/38]\t Loss 0.0336(avg: 0.0636)\t Acc 0.983 (avg: 0.978)\n",
            "Epoch: [20][8/38]\t Loss 0.0198(avg: 0.0587)\t Acc 1.000 (avg: 0.980)\n",
            "Epoch: [20][9/38]\t Loss 0.0125(avg: 0.0542)\t Acc 1.000 (avg: 0.982)\n",
            "Epoch: [20][10/38]\t Loss 0.0079(avg: 0.0499)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [20][11/38]\t Loss 0.0544(avg: 0.0503)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [20][12/38]\t Loss 0.0190(avg: 0.0478)\t Acc 1.000 (avg: 0.985)\n",
            "Epoch: [20][13/38]\t Loss 0.0670(avg: 0.0492)\t Acc 0.968 (avg: 0.984)\n",
            "Epoch: [20][14/38]\t Loss 0.0083(avg: 0.0466)\t Acc 1.000 (avg: 0.985)\n",
            "Epoch: [20][15/38]\t Loss 0.0241(avg: 0.0452)\t Acc 1.000 (avg: 0.986)\n",
            "Epoch: [20][16/38]\t Loss 0.0460(avg: 0.0452)\t Acc 0.984 (avg: 0.986)\n",
            "Epoch: [20][17/38]\t Loss 0.0043(avg: 0.0429)\t Acc 1.000 (avg: 0.987)\n",
            "Epoch: [20][18/38]\t Loss 0.0414(avg: 0.0429)\t Acc 0.984 (avg: 0.986)\n",
            "Epoch: [20][19/38]\t Loss 0.0234(avg: 0.0419)\t Acc 1.000 (avg: 0.987)\n",
            "Epoch: [20][20/38]\t Loss 0.0221(avg: 0.0409)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [20][21/38]\t Loss 0.0554(avg: 0.0416)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [20][22/38]\t Loss 0.0350(avg: 0.0413)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [20][23/38]\t Loss 0.0249(avg: 0.0407)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [20][24/38]\t Loss 0.0084(avg: 0.0394)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [20][25/38]\t Loss 0.0165(avg: 0.0385)\t Acc 1.000 (avg: 0.989)\n",
            "Epoch: [20][26/38]\t Loss 0.1300(avg: 0.0419)\t Acc 0.984 (avg: 0.989)\n",
            "Epoch: [20][27/38]\t Loss 0.0957(avg: 0.0439)\t Acc 0.984 (avg: 0.989)\n",
            "Epoch: [20][28/38]\t Loss 0.0983(avg: 0.0458)\t Acc 0.968 (avg: 0.988)\n",
            "Epoch: [20][29/38]\t Loss 0.0413(avg: 0.0456)\t Acc 0.968 (avg: 0.987)\n",
            "Epoch: [20][30/38]\t Loss 0.0182(avg: 0.0447)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [20][31/38]\t Loss 0.0216(avg: 0.0440)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [20][32/38]\t Loss 0.0035(avg: 0.0428)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [20][33/38]\t Loss 0.0441(avg: 0.0428)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [20][34/38]\t Loss 0.0253(avg: 0.0423)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [20][35/38]\t Loss 0.1494(avg: 0.0454)\t Acc 0.969 (avg: 0.988)\n",
            "Epoch: [20][36/38]\t Loss 0.0673(avg: 0.0459)\t Acc 0.984 (avg: 0.987)\n",
            "Epoch: [20][37/38]\t Loss 0.2859(avg: 0.0465)\t Acc 0.800 (avg: 0.987)\n",
            "Epoch: [20]\t Avg Loss 0.0465\t Avg Accuracy 0.987\n",
            "Test Average Accuracy: 0.7800\n",
            "Epoch: [21][0/38]\t Loss 0.0767(avg: 0.0767)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [21][1/38]\t Loss 0.0370(avg: 0.0573)\t Acc 0.967 (avg: 0.976)\n",
            "Epoch: [21][2/38]\t Loss 0.0118(avg: 0.0419)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [21][3/38]\t Loss 0.0536(avg: 0.0448)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [21][4/38]\t Loss 0.0499(avg: 0.0458)\t Acc 0.969 (avg: 0.981)\n",
            "Epoch: [21][5/38]\t Loss 0.0353(avg: 0.0441)\t Acc 0.983 (avg: 0.981)\n",
            "Epoch: [21][6/38]\t Loss 0.0200(avg: 0.0407)\t Acc 0.984 (avg: 0.981)\n",
            "Epoch: [21][7/38]\t Loss 0.1549(avg: 0.0553)\t Acc 0.968 (avg: 0.980)\n",
            "Epoch: [21][8/38]\t Loss 0.0066(avg: 0.0500)\t Acc 1.000 (avg: 0.982)\n",
            "Epoch: [21][9/38]\t Loss 0.0739(avg: 0.0524)\t Acc 0.984 (avg: 0.982)\n",
            "Epoch: [21][10/38]\t Loss 0.0403(avg: 0.0512)\t Acc 0.984 (avg: 0.982)\n",
            "Epoch: [21][11/38]\t Loss 0.2630(avg: 0.0689)\t Acc 0.968 (avg: 0.981)\n",
            "Epoch: [21][12/38]\t Loss 0.0368(avg: 0.0665)\t Acc 0.984 (avg: 0.981)\n",
            "Epoch: [21][13/38]\t Loss 0.0643(avg: 0.0663)\t Acc 0.984 (avg: 0.982)\n",
            "Epoch: [21][14/38]\t Loss 0.0285(avg: 0.0637)\t Acc 0.984 (avg: 0.982)\n",
            "Epoch: [21][15/38]\t Loss 0.0112(avg: 0.0604)\t Acc 1.000 (avg: 0.983)\n",
            "Epoch: [21][16/38]\t Loss 0.0479(avg: 0.0597)\t Acc 0.984 (avg: 0.983)\n",
            "Epoch: [21][17/38]\t Loss 0.0041(avg: 0.0566)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [21][18/38]\t Loss 0.1204(avg: 0.0599)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [21][19/38]\t Loss 0.0371(avg: 0.0587)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [21][20/38]\t Loss 0.0084(avg: 0.0563)\t Acc 1.000 (avg: 0.985)\n",
            "Epoch: [21][21/38]\t Loss 0.0293(avg: 0.0551)\t Acc 0.984 (avg: 0.985)\n",
            "Epoch: [21][22/38]\t Loss 0.0806(avg: 0.0562)\t Acc 0.967 (avg: 0.984)\n",
            "Epoch: [21][23/38]\t Loss 0.1125(avg: 0.0585)\t Acc 0.952 (avg: 0.983)\n",
            "Epoch: [21][24/38]\t Loss 0.0139(avg: 0.0567)\t Acc 1.000 (avg: 0.983)\n",
            "Epoch: [21][25/38]\t Loss 0.1301(avg: 0.0596)\t Acc 0.969 (avg: 0.983)\n",
            "Epoch: [21][26/38]\t Loss 0.0330(avg: 0.0586)\t Acc 0.984 (avg: 0.983)\n",
            "Epoch: [21][27/38]\t Loss 0.0826(avg: 0.0595)\t Acc 0.968 (avg: 0.982)\n",
            "Epoch: [21][28/38]\t Loss 0.0030(avg: 0.0575)\t Acc 1.000 (avg: 0.983)\n",
            "Epoch: [21][29/38]\t Loss 0.0754(avg: 0.0581)\t Acc 0.984 (avg: 0.983)\n",
            "Epoch: [21][30/38]\t Loss 0.0155(avg: 0.0568)\t Acc 1.000 (avg: 0.983)\n",
            "Epoch: [21][31/38]\t Loss 0.0579(avg: 0.0568)\t Acc 0.969 (avg: 0.983)\n",
            "Epoch: [21][32/38]\t Loss 0.0688(avg: 0.0572)\t Acc 0.984 (avg: 0.983)\n",
            "Epoch: [21][33/38]\t Loss 0.0385(avg: 0.0566)\t Acc 0.984 (avg: 0.983)\n",
            "Epoch: [21][34/38]\t Loss 0.0099(avg: 0.0553)\t Acc 1.000 (avg: 0.983)\n",
            "Epoch: [21][35/38]\t Loss 0.0120(avg: 0.0541)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [21][36/38]\t Loss 0.0062(avg: 0.0527)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [21][37/38]\t Loss 0.0000(avg: 0.0526)\t Acc 1.000 (avg: 0.984)\n",
            "Epoch: [21]\t Avg Loss 0.0526\t Avg Accuracy 0.984\n",
            "Test Average Accuracy: 0.7879\n",
            "Epoch: [22][0/38]\t Loss 0.0403(avg: 0.0403)\t Acc 0.984 (avg: 0.984)\n",
            "Epoch: [22][1/38]\t Loss 0.0107(avg: 0.0255)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [22][2/38]\t Loss 0.0171(avg: 0.0227)\t Acc 1.000 (avg: 0.995)\n",
            "Epoch: [22][3/38]\t Loss 0.0148(avg: 0.0207)\t Acc 1.000 (avg: 0.996)\n",
            "Epoch: [22][4/38]\t Loss 0.0347(avg: 0.0235)\t Acc 0.984 (avg: 0.994)\n",
            "Epoch: [22][5/38]\t Loss 0.0051(avg: 0.0205)\t Acc 1.000 (avg: 0.995)\n",
            "Epoch: [22][6/38]\t Loss 0.0343(avg: 0.0224)\t Acc 0.984 (avg: 0.993)\n",
            "Epoch: [22][7/38]\t Loss 0.0337(avg: 0.0238)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [22][8/38]\t Loss 0.0105(avg: 0.0224)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [22][9/38]\t Loss 0.0385(avg: 0.0240)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [22][10/38]\t Loss 0.0655(avg: 0.0278)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [22][11/38]\t Loss 0.0207(avg: 0.0272)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [22][12/38]\t Loss 0.0011(avg: 0.0253)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [22][13/38]\t Loss 0.0449(avg: 0.0266)\t Acc 0.983 (avg: 0.992)\n",
            "Epoch: [22][14/38]\t Loss 0.0079(avg: 0.0254)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [22][15/38]\t Loss 0.0101(avg: 0.0244)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [22][16/38]\t Loss 0.0329(avg: 0.0249)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [22][17/38]\t Loss 0.0123(avg: 0.0242)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [22][18/38]\t Loss 0.0656(avg: 0.0264)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [22][19/38]\t Loss 0.0072(avg: 0.0254)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [22][20/38]\t Loss 0.0968(avg: 0.0288)\t Acc 0.968 (avg: 0.992)\n",
            "Epoch: [22][21/38]\t Loss 0.1058(avg: 0.0323)\t Acc 0.967 (avg: 0.991)\n",
            "Epoch: [22][22/38]\t Loss 0.0326(avg: 0.0323)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [22][23/38]\t Loss 0.0332(avg: 0.0323)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [22][24/38]\t Loss 0.0677(avg: 0.0337)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [22][25/38]\t Loss 0.0056(avg: 0.0326)\t Acc 1.000 (avg: 0.990)\n",
            "Epoch: [22][26/38]\t Loss 0.0509(avg: 0.0333)\t Acc 0.968 (avg: 0.989)\n",
            "Epoch: [22][27/38]\t Loss 0.0334(avg: 0.0333)\t Acc 0.983 (avg: 0.989)\n",
            "Epoch: [22][28/38]\t Loss 0.0451(avg: 0.0337)\t Acc 0.968 (avg: 0.988)\n",
            "Epoch: [22][29/38]\t Loss 0.0244(avg: 0.0334)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [22][30/38]\t Loss 0.0237(avg: 0.0331)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [22][31/38]\t Loss 0.0865(avg: 0.0348)\t Acc 0.952 (avg: 0.987)\n",
            "Epoch: [22][32/38]\t Loss 0.0199(avg: 0.0343)\t Acc 1.000 (avg: 0.987)\n",
            "Epoch: [22][33/38]\t Loss 0.0694(avg: 0.0354)\t Acc 0.968 (avg: 0.987)\n",
            "Epoch: [22][34/38]\t Loss 0.1629(avg: 0.0390)\t Acc 0.935 (avg: 0.985)\n",
            "Epoch: [22][35/38]\t Loss 0.0171(avg: 0.0384)\t Acc 0.984 (avg: 0.985)\n",
            "Epoch: [22][36/38]\t Loss 0.0089(avg: 0.0376)\t Acc 1.000 (avg: 0.986)\n",
            "Epoch: [22][37/38]\t Loss 0.0064(avg: 0.0375)\t Acc 1.000 (avg: 0.986)\n",
            "Epoch: [22]\t Avg Loss 0.0375\t Avg Accuracy 0.986\n",
            "Test Average Accuracy: 0.7774\n",
            "Epoch: [23][0/38]\t Loss 0.0170(avg: 0.0170)\t Acc 1.000 (avg: 1.000)\n",
            "Epoch: [23][1/38]\t Loss 0.1647(avg: 0.0914)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [23][2/38]\t Loss 0.0235(avg: 0.0689)\t Acc 1.000 (avg: 0.995)\n",
            "Epoch: [23][3/38]\t Loss 0.0538(avg: 0.0651)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [23][4/38]\t Loss 0.0247(avg: 0.0570)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][5/38]\t Loss 0.0150(avg: 0.0501)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [23][6/38]\t Loss 0.0047(avg: 0.0436)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [23][7/38]\t Loss 0.0341(avg: 0.0424)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [23][8/38]\t Loss 0.0249(avg: 0.0406)\t Acc 0.966 (avg: 0.989)\n",
            "Epoch: [23][9/38]\t Loss 0.0012(avg: 0.0366)\t Acc 1.000 (avg: 0.990)\n",
            "Epoch: [23][10/38]\t Loss 0.0477(avg: 0.0376)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [23][11/38]\t Loss 0.0195(avg: 0.0362)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][12/38]\t Loss 0.0237(avg: 0.0352)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [23][13/38]\t Loss 0.0022(avg: 0.0328)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][14/38]\t Loss 0.0017(avg: 0.0307)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][15/38]\t Loss 0.0240(avg: 0.0303)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][16/38]\t Loss 0.0221(avg: 0.0298)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][17/38]\t Loss 0.0057(avg: 0.0285)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][18/38]\t Loss 0.0272(avg: 0.0284)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][19/38]\t Loss 0.0895(avg: 0.0316)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [23][20/38]\t Loss 0.0621(avg: 0.0330)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [23][21/38]\t Loss 0.0075(avg: 0.0318)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][22/38]\t Loss 0.0180(avg: 0.0313)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][23/38]\t Loss 0.2605(avg: 0.0411)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][24/38]\t Loss 0.0025(avg: 0.0395)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][25/38]\t Loss 0.0121(avg: 0.0384)\t Acc 1.000 (avg: 0.991)\n",
            "Epoch: [23][26/38]\t Loss 0.0026(avg: 0.0371)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [23][27/38]\t Loss 0.0383(avg: 0.0372)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][28/38]\t Loss 0.0048(avg: 0.0360)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [23][29/38]\t Loss 0.0553(avg: 0.0367)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][30/38]\t Loss 0.0313(avg: 0.0365)\t Acc 0.984 (avg: 0.991)\n",
            "Epoch: [23][31/38]\t Loss 0.0528(avg: 0.0370)\t Acc 0.967 (avg: 0.990)\n",
            "Epoch: [23][32/38]\t Loss 0.0703(avg: 0.0380)\t Acc 0.984 (avg: 0.990)\n",
            "Epoch: [23][33/38]\t Loss 0.1696(avg: 0.0418)\t Acc 0.934 (avg: 0.989)\n",
            "Epoch: [23][34/38]\t Loss 0.0661(avg: 0.0425)\t Acc 0.969 (avg: 0.988)\n",
            "Epoch: [23][35/38]\t Loss 0.0002(avg: 0.0414)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [23][36/38]\t Loss 0.0242(avg: 0.0409)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [23][37/38]\t Loss 0.0035(avg: 0.0408)\t Acc 1.000 (avg: 0.988)\n",
            "Epoch: [23]\t Avg Loss 0.0408\t Avg Accuracy 0.988\n",
            "Test Average Accuracy: 0.7833\n",
            "Epoch: [24][0/38]\t Loss 0.0075(avg: 0.0075)\t Acc 1.000 (avg: 1.000)\n",
            "Epoch: [24][1/38]\t Loss 0.1355(avg: 0.0705)\t Acc 0.968 (avg: 0.984)\n",
            "Epoch: [24][2/38]\t Loss 0.0048(avg: 0.0491)\t Acc 1.000 (avg: 0.989)\n",
            "Epoch: [24][3/38]\t Loss 0.0261(avg: 0.0432)\t Acc 0.984 (avg: 0.988)\n",
            "Epoch: [24][4/38]\t Loss 0.0018(avg: 0.0349)\t Acc 1.000 (avg: 0.990)\n",
            "Epoch: [24][5/38]\t Loss 0.0071(avg: 0.0303)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [24][6/38]\t Loss 0.0129(avg: 0.0278)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][7/38]\t Loss 0.0327(avg: 0.0284)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [24][8/38]\t Loss 0.0008(avg: 0.0254)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][9/38]\t Loss 0.0514(avg: 0.0281)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [24][10/38]\t Loss 0.0008(avg: 0.0256)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][11/38]\t Loss 0.0321(avg: 0.0261)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][12/38]\t Loss 0.0081(avg: 0.0247)\t Acc 1.000 (avg: 0.994)\n",
            "Epoch: [24][13/38]\t Loss 0.0043(avg: 0.0233)\t Acc 1.000 (avg: 0.994)\n",
            "Epoch: [24][14/38]\t Loss 0.0502(avg: 0.0252)\t Acc 0.969 (avg: 0.993)\n",
            "Epoch: [24][15/38]\t Loss 0.0166(avg: 0.0246)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][16/38]\t Loss 0.0352(avg: 0.0252)\t Acc 0.968 (avg: 0.992)\n",
            "Epoch: [24][17/38]\t Loss 0.0022(avg: 0.0240)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [24][18/38]\t Loss 0.0139(avg: 0.0234)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [24][19/38]\t Loss 0.0151(avg: 0.0230)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][20/38]\t Loss 0.0045(avg: 0.0221)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][21/38]\t Loss 0.0383(avg: 0.0228)\t Acc 0.983 (avg: 0.993)\n",
            "Epoch: [24][22/38]\t Loss 0.0065(avg: 0.0221)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][23/38]\t Loss 0.0071(avg: 0.0215)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][24/38]\t Loss 0.0169(avg: 0.0213)\t Acc 1.000 (avg: 0.994)\n",
            "Epoch: [24][25/38]\t Loss 0.0433(avg: 0.0221)\t Acc 0.984 (avg: 0.993)\n",
            "Epoch: [24][26/38]\t Loss 0.0491(avg: 0.0231)\t Acc 0.984 (avg: 0.993)\n",
            "Epoch: [24][27/38]\t Loss 0.0875(avg: 0.0254)\t Acc 0.984 (avg: 0.993)\n",
            "Epoch: [24][28/38]\t Loss 0.0026(avg: 0.0246)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][29/38]\t Loss 0.0204(avg: 0.0244)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][30/38]\t Loss 0.0414(avg: 0.0250)\t Acc 0.968 (avg: 0.992)\n",
            "Epoch: [24][31/38]\t Loss 0.0059(avg: 0.0244)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [24][32/38]\t Loss 0.0067(avg: 0.0239)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][33/38]\t Loss 0.0017(avg: 0.0232)\t Acc 1.000 (avg: 0.993)\n",
            "Epoch: [24][34/38]\t Loss 0.0596(avg: 0.0242)\t Acc 0.951 (avg: 0.992)\n",
            "Epoch: [24][35/38]\t Loss 0.0276(avg: 0.0243)\t Acc 0.984 (avg: 0.992)\n",
            "Epoch: [24][36/38]\t Loss 0.0056(avg: 0.0238)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [24][37/38]\t Loss 0.0610(avg: 0.0239)\t Acc 1.000 (avg: 0.992)\n",
            "Epoch: [24]\t Avg Loss 0.0239\t Avg Accuracy 0.992\n",
            "Test Average Accuracy: 0.7768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing and visualize prediction"
      ],
      "metadata": {
        "id": "hkzwh5Wpa3XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import webbrowser"
      ],
      "metadata": {
        "id": "51-y8cZMvKmt"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the best model we've trained**"
      ],
      "metadata": {
        "id": "JOuEfj2KbD3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint = torch.load(\"best_model/model.pth.tar\")\n",
        "model = checkpoint['model']\n",
        "model.eval()\n",
        "\n",
        "dataset = News20Dataset(\"glove.6B.100d.txt\", is_train=False)\n",
        "doc = \"Amidst this cosmic symphony, instruments of perception extend our senses beyond the limitations of our mortal coil. We gaze through lenses of glass and metal, peering into realms unseen, where galaxies spiral in cosmic embrace. Signals, like ethereal messengers, traverse the void, carrying secrets encoded in the language of pulses and waves.\"\n",
        "\n",
        "result = visualize(model, dataset, doc)\n",
        "\n",
        "with open('result.html', 'w') as f:\n",
        "    f.write(result)\n",
        "\n",
        "webbrowser.open_new('file://'+os.getcwd()+'/result.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "ubJlx18ORpCs",
        "outputId": "06919987-c623-4e50-9896-a33fef59a6c7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-b713a7c3a741>:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  sentencemap = matplotlib.cm.get_cmap('binary')\n",
            "<ipython-input-52-b713a7c3a741>:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  wordmap = matplotlib.cm.get_cmap('OrRd')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Read the HTML file content\n",
        "with open('result.html', 'r') as file:\n",
        "    html_content = file.read()\n",
        "\n",
        "# Display the HTML content\n",
        "display(HTML(html_content))\n"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/prediction_bar_chart.png": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "XVHUd28fytdR",
        "outputId": "29ecd5d2-6d14-40a2-9c63-ea79e9cf8175"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>Attention Visualization</h2><br><img src=\"prediction_bar_chart.png\"><br><p><span style=\"margin:5px; padding:5px; background-color: #010101\"><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspAmidst&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspthis&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff5e7\">&nbspcosmic&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspsymphony&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspinstruments&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspperception&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspextend&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspour&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspsenses&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspbeyond&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspthe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsplimitations&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspour&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fc9964\">&nbspmortal&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fb8b58\">&nbspcoil&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsp.&nbsp</span></span><p><p><span style=\"margin:5px; padding:5px; background-color: #fefefe\"><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspWe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspgaze&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspthrough&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fee8c9\">&nbsplenses&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff5e7\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspglass&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspand&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspmetal&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff5e7\">&nbsppeering&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7eb\">&nbspinto&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fdbb85\">&nbsprealms&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feedd4\">&nbspunseen&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspwhere&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feeace\">&nbspgalaxies&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff2e1\">&nbspspiral&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7eb\">&nbspin&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fddaab\">&nbspcosmic&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspembrace&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsp.&nbsp</span></span><p><p><span style=\"margin:5px; padding:5px; background-color: #ffffff\"><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspSignals&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsplike&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff2e0\">&nbspethereal&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fddeb3\">&nbspmessengers&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff0db\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff5e7\">&nbsptraverse&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspthe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspvoid&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7eb\">&nbsp,&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7eb\">&nbspcarrying&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feeed5\">&nbspsecrets&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #feecd2\">&nbspencoded&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspin&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspthe&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsplanguage&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7eb\">&nbspof&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fc8f5b\">&nbsppulses&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7eb\">&nbspand&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbspwaves&nbsp</span><span class=\"barcode\"; style=\"color: black; background-color: #fff7ec\">&nbsp.&nbsp</span></span><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}